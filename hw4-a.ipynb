{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c3110823",
      "metadata": {
        "id": "c3110823"
      },
      "source": [
        "## HW4 (a) Overview (30 points + bonus)\n",
        "\n",
        "This assignment explores the phonetic and acoustic information is captured at different layers of the Hubert model through **probing experiments**. We'll train simple classifiers and regressors to predict various targets from Hubert features:\n",
        "\n",
        "1. **Phoneme Recognition**: Train a classifier to predict phone labels from Hubert features\n",
        "2. **MFCC Regression**: Train a regressor to predict MFCC features from Hubert features  \n",
        "3. **Speaker Embedding**: Analyze speaker separation in Hubert features using t-SNE\n",
        "4. **Bonus question** - Create your own speaker embedding\n",
        "\n",
        "Note - You should use a GPU for this part as well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c23390b2",
      "metadata": {
        "id": "c23390b2"
      },
      "source": [
        "## What is Probing?\n",
        "\n",
        "**Probing** is an interpretability technique to understand what information is encoded in different layers of a neural network. We train simple models (probes) to predict specific targets from the network's internal representations. If a probe can successfully predict a target, it suggests that information is encoded in those representations. Note that when training probes, the aim is to not maximize on accuracy but reflect the true information content in features, and hence you should not overparameterize your classifier.\n",
        "\n",
        "**Key Questions**:\n",
        "- What phonetic information is captured at each layer?\n",
        "- How does the model's understanding evolve from acoustic to phonetics through the layers?\n",
        "- Can we identify speaker-specific patterns in the features?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8478f409",
      "metadata": {
        "id": "8478f409"
      },
      "source": [
        "## Submission\n",
        "\n",
        "1. Please submit a PDF of the completed notebook with all outputs.\n",
        "2. Additionally, submit  a PDF REPORT with the plots and deliverables (including answers to the questions).\n",
        "\n",
        "NOTE - Only the pdf report will be graded, although we may look at the notebook to verify solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45dbed77",
      "metadata": {
        "id": "45dbed77"
      },
      "source": [
        "### Task 1: Phoneme Recognition (Classification) [10 Points]\n",
        "\n",
        "**Objective**: Train a classifier to predict phone labels from Hubert features. This tests how well each layer captures **phonetic information**.\n",
        "\n",
        "**Task Overview**: In this task, you will implement a phoneme recognition probe to understand how phonetic information is encoded across different layers of the Hubert model. You'll load pre-computed Hubert features and corresponding phone labels for each layer (0-12), then train a simple 2-layer neural network classifier to predict phone categories from these features. Your goal is to investigate whether different layers of the model contain varying amounts of phonetic information, and if so, how this information is distributed across the model's depth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "642021b9",
      "metadata": {
        "id": "642021b9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "35334b81",
      "metadata": {
        "id": "35334b81",
        "outputId": "c22c4d9c-fb19-46e4-b1e2-7c4bb781d745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7ad0795",
      "metadata": {
        "id": "d7ad0795"
      },
      "source": [
        "**Download phoneme recognition data using the link below**\n",
        "https://drive.google.com/file/d/1UuABo6aDByR61DPcESzc6jm5aMAAOM2Y/view?usp=sharing\n",
        "\n",
        "**Setting up data and Mounting Drive**\n",
        "To avoid loosing your data as collab runtime disconnects, you should mount your google drive here so that you are able to use the same data folder. An example of how to mount drive is given in the hw4 part b notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting your Google drive to Colab environment.\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Mounted at /content/drive\n",
        "\n",
        "data_zip = '/content/drive/MyDrive/Colab Notebooks/phoneme_layer_datasets.zip'\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(data_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/MyDrive/183dataset/')\n",
        "\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/YOUR_DIR/DATA-FOLDER-NAME\"\n",
        "\n"
      ],
      "metadata": {
        "id": "FzwTSKGfABx4",
        "outputId": "dfbd1368-685f-44c2-864d-99e1427af11e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FzwTSKGfABx4",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "339c2672",
      "metadata": {
        "id": "339c2672",
        "outputId": "e2b7f96a-d1ea-4e90-cdb4-9c20b0bbb9b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'phoneme_layer_datasets/layer_0/train_features.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1496321599.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mbase_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"phoneme_layer_datasets\"\u001b[0m \u001b[0;31m#TODO : Change this to the correct base folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mlayer_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_phoneme_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Layer {layer_idx} data loaded successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1496321599.py\u001b[0m in \u001b[0;36mload_phoneme_data\u001b[0;34m(base_folder, layer_idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Load features (PyTorch tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_features.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdev_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dev_features.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_features.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'phoneme_layer_datasets/layer_0/train_features.pt'"
          ]
        }
      ],
      "source": [
        "def load_phoneme_data(base_folder, layer_idx):\n",
        "    \"\"\"\n",
        "    Helper function to load phoneme data for a specific layer.\n",
        "    This function loads the features and labels for a specific layer from the base folder.\n",
        "    The features are loaded as PyTorch tensors and the labels are loaded as JSON files.\n",
        "    The labels are already string phone names.\n",
        "    Use this function to load the data for a specific layer.\n",
        "    We recommend you to play around with the loaded data to understand the format and the information contained.\n",
        "    \"\"\"\n",
        "    base_path = os.path.join(base_folder, f\"layer_{layer_idx}\")\n",
        "\n",
        "    # Load features (PyTorch tensors)\n",
        "    train_features = torch.load(os.path.join(base_path, 'train_features.pt'))\n",
        "    dev_features = torch.load(os.path.join(base_path, 'dev_features.pt'))\n",
        "    test_features = torch.load(os.path.join(base_path, 'test_features.pt'))\n",
        "\n",
        "    # Load labels (JSON files with 'labels' key)\n",
        "    with open(os.path.join(base_path, 'train_labels.json'), 'r') as f:\n",
        "        train_labels_data = json.load(f)\n",
        "    with open(os.path.join(base_path, 'dev_labels.json'), 'r') as f:\n",
        "        dev_labels_data = json.load(f)\n",
        "    with open(os.path.join(base_path, 'test_labels.json'), 'r') as f:\n",
        "        test_labels_data = json.load(f)\n",
        "\n",
        "    # Extract labels from JSON structure (labels are already string phone names)\n",
        "    train_labels = train_labels_data['labels']\n",
        "    dev_labels = dev_labels_data['labels']\n",
        "    test_labels = test_labels_data['labels']\n",
        "\n",
        "    # Create vocabulary from all unique labels in the dataset\n",
        "    all_labels = train_labels + dev_labels + test_labels\n",
        "    unique_labels = sorted(list(set(all_labels)))\n",
        "    phone_vocab = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "    print(f\"Created vocabulary with {len(unique_labels)} unique phone classes:\")\n",
        "    print(f\"Sample phones: {unique_labels[:10]}\")\n",
        "\n",
        "    # Convert string phone labels to integer indices using our vocabulary\n",
        "    train_labels = [phone_vocab[label] for label in train_labels]\n",
        "    dev_labels = [phone_vocab[label] for label in dev_labels]\n",
        "    test_labels = [phone_vocab[label] for label in test_labels]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "    dev_labels = torch.tensor(dev_labels, dtype=torch.long)\n",
        "    test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "    return train_features, train_labels, dev_features, dev_labels, test_features, test_labels\n",
        "\n",
        "# Test loading data for layer 0\n",
        "base_folder = \"phoneme_layer_datasets\" #TODO : Change this to the correct base folder.\n",
        "layer_idx = 0\n",
        "train_features, train_labels, dev_features, dev_labels, test_features, test_labels = load_phoneme_data(base_folder, layer_idx)\n",
        "\n",
        "print(f\"Layer {layer_idx} data loaded successfully!\")\n",
        "print(f\"Train features shape: {train_features.shape}\")\n",
        "print(f\"Train labels shape: {train_labels.shape}\")\n",
        "print(f\"Dev features shape: {dev_features.shape}\")\n",
        "print(f\"Test features shape: {test_features.shape}\")\n",
        "print(f\"Number of unique phone classes: {len(torch.unique(train_labels))}\")\n",
        "\n",
        "\"\"\"\n",
        "Recommended : Play around with the loaded data to understand the information contained in it.\n",
        "TODO : Figure out the feature dimensionality based on the loaded data. From that, try to figure out which version of Hubert is being used.\n",
        "\"\"\"\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe310f6",
      "metadata": {
        "id": "0fe310f6"
      },
      "outputs": [],
      "source": [
        "class Probe(nn.Module):\n",
        "    \"\"\"Generic 2-layer probe for both classification and regression tasks.\n",
        "    Make sure to use the same architecture for both classification and regression tasks.\n",
        "    The architecture should have 2 layers, with intermediate hidden dimension of 128.\n",
        "    The output layer should have the same dimension as the number of classes in the dataset.\n",
        "    In between the two layers, use a ReLU activation function.\n",
        "\n",
        "    Note : Follow the above architecture detail exactly. Do not try to overparameterize the probe.\n",
        "            You are not graded on the accuracy of the probe but on the interpretability of the results.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=768, hidden_dim=128, output_dim=50,):\n",
        "        super(Probe, self).__init__()\n",
        "\n",
        "        # Same 2-layer architecture for both classification and regression\n",
        "        #self.network = TODO\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec58951",
      "metadata": {
        "id": "8ec58951"
      },
      "outputs": [],
      "source": [
        "def train_phoneme_probe(base_folder, layer_idx, num_epochs=10, batch_size=512, learning_rate=0.001):\n",
        "    \"\"\"Train phoneme probe for a specific layer.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training Phoneme Probe for Layer {layer_idx}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Load data\n",
        "    train_features, train_labels, dev_features, dev_labels, test_features, test_labels = load_phoneme_data(base_folder, layer_idx)\n",
        "\n",
        "    # Get number of classes\n",
        "    num_classes = len(torch.unique(train_labels))\n",
        "    print(f\"Number of phone classes: {num_classes}\")\n",
        "    print(f\"Training samples: {len(train_features)}\")\n",
        "    print(f\"Dev samples: {len(dev_features)}\")\n",
        "    print(f\"Test samples: {len(test_features)}\")\n",
        "\n",
        "    # Standardize features\n",
        "    # TODO : Standardize the features using the mean and standard deviation of the training features.\n",
        "\n",
        "    # Move to device\n",
        "    # TODO : Move the features and labels to the device.\n",
        "\n",
        "    # Create model\n",
        "    # TODO : Create the model using the Probe class. Use cross entropy loss and Adam optimizer.\n",
        "\n",
        "    # Create data loaders\n",
        "    # TODO : Create the data loaders using the TensorDataset and DataLoader classes.\n",
        "\n",
        "    # Training loop\n",
        "    # TODO : Implement the training loop.\n",
        "\n",
        "    # Test evaluation\n",
        "    # TODO : Implement the test evaluation.\n",
        "\n",
        "    # Return the results.\n",
        "    # TODO : Be sure to return the test accuracy for each layer. You should plot the test accuracy for each layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b79517",
      "metadata": {
        "id": "b4b79517"
      },
      "outputs": [],
      "source": [
        "### Task 1 : Training the probes\n",
        "## This code represents an example how the probes are trained.\n",
        "## You can use this as a reference to train your probes, plot the results.\n",
        "## You don't have to follow this exact code. Please feel free to modify as per your implementation of the train_phoneme_probe function.\n",
        "\n",
        "\n",
        "# Train phoneme probes for all layers\n",
        "base_folder = \"layer_datasets\"\n",
        "num_epochs = 5\n",
        "batch_size = 512\n",
        "\n",
        "# Store results for all layers\n",
        "all_results = []\n",
        "\n",
        "print(\"Training phoneme probes for all layers (0-12)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for layer_idx in range(13):\n",
        "    print(f\"\\nTraining Layer {layer_idx}...\")\n",
        "    result = train_phoneme_probe(base_folder, layer_idx, num_epochs, batch_size)\n",
        "    all_results.append(result)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING COMPLETED FOR ALL LAYERS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Print test accuracy for each layer\n",
        "print(\"\\nTest Accuracy by Layer:\")\n",
        "print(\"-\" * 30)\n",
        "for result in all_results:\n",
        "    layer = result['layer']\n",
        "    test_acc = result['test_acc']\n",
        "    print(f\"Layer {layer:2d}: {test_acc:.4f}\")\n",
        "\n",
        "print(f\"\\nBest performing layer: {max(all_results, key=lambda x: x['test_acc'])['layer']}\")\n",
        "print(f\"Best test accuracy: {max(all_results, key=lambda x: x['test_acc'])['test_acc']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7520e091",
      "metadata": {
        "id": "7520e091"
      },
      "source": [
        "### Task 1: Deliverables\n",
        "\n",
        "1. Plot of phoneme recognition accuracy on y-axis vs layers (5 points)\n",
        "\n",
        "2. Answer the following questions:\n",
        "    \n",
        "    2.1 The features used in this task belongs to which version of the Hubert model ? (1 point)\n",
        "\n",
        "    2.2 What layer features does best in the phoneme recognition task? (1 point)\n",
        "\n",
        "    2.3 What does this plot show about how phonetic information is captured by Hubert? Why do you think that's the case? (3 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014131ae",
      "metadata": {
        "id": "014131ae"
      },
      "source": [
        "### Task 2: MFCC Regression [10 points]\n",
        "\n",
        "**Objective**: Train a regressor to predict MFCC features from Hubert features. This tests how well each layer captures **acoustic information**.\n",
        "\n",
        "**What are MFCC Features?**\n",
        "\n",
        "**MFCC (Mel-Frequency Cepstral Coefficients)** are acoustic features that capture the spectral characteristics of speech. MFCCs are computed by:\n",
        "1. **Windowing** the audio signal into short frames (typically 25ms)\n",
        "2. **Computing FFT** to get frequency spectrum\n",
        "3. **Mel-scale filtering** to simulate human auditory perception\n",
        "4. **Log transformation** to compress dynamic range\n",
        "5. **DCT (Discrete Cosine Transform)** to decorrelate coefficients\n",
        "\n",
        "The 13 MFCC coefficients capture different aspects of the acoustic signal, with lower coefficients representing broader spectral characteristics and higher coefficients capturing finer details.\n",
        "\n",
        "**Task Overview**: In this task, you will implement an **MFCC regression probe** to understand how acoustic information is encoded across different layers of the Hubert model. You'll load pre-computed Hubert features and corresponding MFCC labels for each layer (0-12), then train a regression probe using the previously defined probe to predict 13 MFCC coefficients from these features. Your goal is to investigate whether different layers of the model contain varying amounts of acoustic information, and if so, how this information is distributed across the model's depth. You'll train separate regressors for each of the 13 layers, evaluate them on held-out test sets using R² scores, and analyze the results to understand what Hubert has learned about acoustic properties at each stage of processing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a382b5ac",
      "metadata": {
        "id": "a382b5ac"
      },
      "source": [
        "**Download mfcc dataset using this link**\n",
        "https://drive.google.com/file/d/12ikp8GxiU7Ul3i5fR-9Sz2YMt9Jdshv4/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e831fb2",
      "metadata": {
        "id": "8e831fb2"
      },
      "outputs": [],
      "source": [
        "def load_mfcc_data(base_folder, layer_idx):\n",
        "    \"\"\"Load MFCC data for a specific layer.\"\"\"\n",
        "    base_path = os.path.join(base_folder, f\"layer_{layer_idx}\")\n",
        "\n",
        "    # Load features (PyTorch tensors)\n",
        "    train_features = torch.load(os.path.join(base_path, 'train_features.pt'))\n",
        "    dev_features = torch.load(os.path.join(base_path, 'dev_features.pt'))\n",
        "    test_features = torch.load(os.path.join(base_path, 'test_features.pt'))\n",
        "\n",
        "    # Load MFCC labels (JSON files with 'mfcc' key)\n",
        "    with open(os.path.join(base_path, 'train_mfcc_labels.json'), 'r') as f:\n",
        "        train_labels_data = json.load(f)\n",
        "    with open(os.path.join(base_path, 'dev_mfcc_labels.json'), 'r') as f:\n",
        "        dev_labels_data = json.load(f)\n",
        "    with open(os.path.join(base_path, 'test_mfcc_labels.json'), 'r') as f:\n",
        "        test_labels_data = json.load(f)\n",
        "\n",
        "    # Extract MFCC labels from JSON structure (labels are 13-element lists)\n",
        "    train_mfcc_labels = train_labels_data['mfcc']  # List of 13-element lists\n",
        "    dev_mfcc_labels = dev_labels_data['mfcc']\n",
        "    test_mfcc_labels = test_labels_data['mfcc']\n",
        "\n",
        "    # Convert to PyTorch tensors (shape: [n_frames, 13])\n",
        "    train_labels = torch.tensor(train_mfcc_labels, dtype=torch.float32)\n",
        "    dev_labels = torch.tensor(dev_mfcc_labels, dtype=torch.float32)\n",
        "    test_labels = torch.tensor(test_mfcc_labels, dtype=torch.float32)\n",
        "\n",
        "    return train_features, train_labels, dev_features, dev_labels, test_features, test_labels\n",
        "\n",
        "# Test loading MFCC data for layer 0\n",
        "base_folder = \"mfcc_layer_datasets\"\n",
        "layer_idx = 0\n",
        "train_features, train_labels, dev_features, dev_labels, test_features, test_labels = load_mfcc_data(base_folder, layer_idx)\n",
        "\n",
        "print(f\"MFCC data loaded for layer {layer_idx}:\")\n",
        "print(f\"Train features shape: {train_features.shape}\")\n",
        "print(f\"Train labels shape: {train_labels.shape}\")\n",
        "print(f\"Dev features shape: {dev_features.shape}\")\n",
        "print(f\"Test features shape: {test_features.shape}\")\n",
        "print(f\"MFCC coefficients: {train_labels.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d57e4d1",
      "metadata": {
        "id": "9d57e4d1"
      },
      "outputs": [],
      "source": [
        "# Corrected MFCC training function\n",
        "def train_mfcc_probe(base_folder, layer_idx, num_epochs=10, batch_size=512, learning_rate=0.001):\n",
        "    \"\"\"Train MFCC probe for a specific layer.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training MFCC Probe for Layer {layer_idx}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Load data using the correct function signature\n",
        "    train_features, train_labels, dev_features, dev_labels, test_features, test_labels = load_mfcc_data(base_folder, layer_idx)\n",
        "\n",
        "    print(f\"Training samples: {len(train_features)}\")\n",
        "    print(f\"Dev samples: {len(dev_features)}\")\n",
        "    print(f\"Test samples: {len(test_features)}\")\n",
        "    print(f\"MFCC dimensions: {train_labels.shape[1]}\")\n",
        "\n",
        "    # Standardize features\n",
        "    # TODO : Standardize the features using the mean and standard deviation of the training features.\n",
        "\n",
        "    # Move to device\n",
        "    # TODO : Move the features and labels to the device.\n",
        "\n",
        "    # Create model using unified Probe class\n",
        "    # TODO : Create the model using the exact same Probe class as before. Use MSE loss and Adam optimizer.\n",
        "\n",
        "    # Create data loaders\n",
        "    # TODO : Create the data loaders using the TensorDataset and DataLoader classes.\n",
        "\n",
        "    # Training loop\n",
        "    # TODO : Implement the training loop.\n",
        "\n",
        "    # Test evaluation\n",
        "    # TODO : Implement the test evaluation. The metric we are looking for is the R^2 metric on the test set.\n",
        "\n",
        "    # Return the results.\n",
        "    # TODO : Be sure to return the test R^2 for each layer. You should plot the test R^2 for each layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28df2d0",
      "metadata": {
        "id": "b28df2d0"
      },
      "outputs": [],
      "source": [
        "### Task 2 : Example code for training MFCC probes\n",
        "## This code represents an example how the probes are trained.\n",
        "## You can use this as a reference to train your probes, plot the results.\n",
        "## You don't have to follow this exact code. Please feel free to modify as per your prior implementations.\n",
        "\n",
        "\n",
        "# Train MFCC probes for all layers\n",
        "base_folder = \"mfcc_layer_datasets\"\n",
        "num_epochs = 5\n",
        "batch_size = 512\n",
        "\n",
        "# Store results for all layers\n",
        "mfcc_all_results = []\n",
        "\n",
        "print(\"Training MFCC probes for all layers (0-12)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for layer_idx in range(13):\n",
        "    print(f\"\\nTraining Layer {layer_idx}...\")\n",
        "    result = train_mfcc_probe(base_folder, layer_idx, num_epochs, batch_size)\n",
        "    mfcc_all_results.append(result)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING COMPLETED FOR ALL LAYERS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Print test R² for each layer\n",
        "print(\"\\nTest R² by Layer:\")\n",
        "print(\"-\" * 30)\n",
        "for result in mfcc_all_results:\n",
        "    layer = result['layer']\n",
        "    test_r2 = result['overall_r2']\n",
        "    print(f\"Layer {layer:2d}: {test_r2:.4f}\")\n",
        "\n",
        "print(f\"\\nBest performing layer: {max(mfcc_all_results, key=lambda x: x['overall_r2'])['layer']}\")\n",
        "print(f\"Best test R²: {max(mfcc_all_results, key=lambda x: x['overall_r2'])['overall_r2']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "074085cf",
      "metadata": {
        "id": "074085cf"
      },
      "source": [
        "### Task 2: Deliverables\n",
        "\n",
        "1. Plot of MFCC regression R^2 metric on y-axis vs layers (5 points)\n",
        "\n",
        "2. Answer the following questions:\n",
        "\n",
        "    2.1 What layer features does best MFCC regression task? (1 point)\n",
        "\n",
        "    2.2 What does layer 0 represent in the Hubert model? (1 point)\n",
        "\n",
        "    2.3 What does this plot show about how acoustic information is captured by Hubert? Why do you think that's the case? (3 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b807d3",
      "metadata": {
        "id": "f8b807d3"
      },
      "source": [
        "### Task 3: Speaker Embedding Analysis [10 points]\n",
        "\n",
        "**Objective**: Analyze how well Hubert features capture **speaker identity** using t-SNE visualization.\n",
        "\n",
        "Speaker embedding analysis investigates how well neural network representations capture speaker-specific information. Unlike phonetic or acoustic features that focus on linguistic content, speaker embeddings capture the unique characteristics that distinguish one speaker from another, such as:\n",
        "\n",
        "- **Voice characteristics**: Fundamental frequency (F0), vocal tract length, speaking rate\n",
        "- **Speaking style**: Prosody, rhythm, intonation patterns  \n",
        "- **Physiological features**: Age, gender, accent, regional variations\n",
        "- **Behavioral patterns**: Individual speaking habits and preferences\n",
        "\n",
        "**Task Overview**: In this task, you will implement a **speaker embedding analysis** to understand how speaker identity information is encoded across different layers of the Hubert model. You will extract audio features from 20 speakers and compute mean-pooled Hubert features from for each layer, and use t-SNE dimensionality reduction to visualize how well different layers separate speakers in the feature space. Your goal is to investigate whether different layers of the model contain varying amounts of speaker information, and if so, how this information is distributed across the model's depth. You'll analyze t-SNE plots for all 13 layers to understand how Hubert's internal representations evolve from speaker-agnostic to speaker-specific information. This experiment will help you understand how Hubert balances linguistic content with speaker identity across its layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114b2fc7",
      "metadata": {
        "id": "114b2fc7"
      },
      "source": [
        "**download speaker embedding data from here**\n",
        "https://drive.google.com/file/d/1L7r1c6_CIPVxNIQVPfrhD_Wt1MKCypU1/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0396a6d",
      "metadata": {
        "id": "d0396a6d"
      },
      "outputs": [],
      "source": [
        "# Import additional libraries for speaker embedding analysis\n",
        "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
        "import torchaudio\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a300d7a3",
      "metadata": {
        "id": "a300d7a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_hubert_model():\n",
        "    \"\"\"\n",
        "    Load the Hubert model and feature extractor.\n",
        "    \"\"\"\n",
        "    print(\"Loading Hubert model...\")\n",
        "    model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n",
        "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
        "    return model, feature_extractor\n",
        "\n",
        "def extract_speaker_embedding(audio_path, model, feature_extractor, layer_idx=0):\n",
        "    \"\"\"\n",
        "    Extract Hubert features from a specific layer.\n",
        "\n",
        "    Args:\n",
        "        audio_path (str): Path to audio file\n",
        "        model: Hubert model\n",
        "        feature_extractor: Hubert feature extractor\n",
        "        layer_idx (int): Layer index (0-12, where 0 is embedding layer)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Features from the specified layer\n",
        "    \"\"\"\n",
        "    # Load and preprocess audio using torchaudio\n",
        "    audio, sr = torchaudio.load(audio_path)\n",
        "\n",
        "    # Resample to 16kHz if needed. Hubert model expects 16kHz audio.\n",
        "\n",
        "    # Extract features from the specified layer.\n",
        "\n",
        "    # TODO : Make sure to do mean-pooling across the time dimension.\n",
        "    ## The speaker embedding from each layer is the mean-pooled features across the time dimension.\n",
        "\n",
        "    # Return the mean-pooled features.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16dd8130",
      "metadata": {
        "id": "16dd8130"
      },
      "outputs": [],
      "source": [
        "# TODO : Go through the dataset and extract the speaker embeddings for each layer.\n",
        "# Audio files are present as 307-127535-0000.flac. The speaker id is 307. All audio files with the same speaker id are from the same speaker.\n",
        "# Each speaker should have 50 audios per speaker. You will thus extract 50 embeddings per speaker for each layer. Do this for all 20 speakers.\n",
        "\n",
        "# TODO : Use t-SNE to visualize the speaker embeddings. We want to see clusters of the embeddings by speaker id.\n",
        "\n",
        "# TODO : Plot the t-SNE plot for all 13 layers separately."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49966835",
      "metadata": {
        "id": "49966835"
      },
      "source": [
        "### Task 3: Deliverables\n",
        "\n",
        "1. Plot the speaker embedding t-SNE plots for each layer. There should be 13 plots. Each plot should have 20 clusters, 1 per speaker, with 50 speaker embeddings per speaker. (5 points)\n",
        "\n",
        "2. Answer the following questions:\n",
        "\n",
        "    2.1 Do speaker embeddings form neat clusters or mix together? (1 point)\n",
        "\n",
        "    2.2 What does this plot show about how speaker information is captured by Hubert? Answer this question based on how the plots evolve as we go deeper into the model. (4 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e488685",
      "metadata": {
        "id": "2e488685"
      },
      "source": [
        "### Bonus: Make your own Speaker Embedding Cluster (10 points)\n",
        "\n",
        "1. Record 5 **different** sentences in your own voice in each of the following settings:\n",
        "\n",
        "   1.1 Use two different mics (For example laptop and phone or something else)\n",
        "\n",
        "   1.2 For each mic, record yourself in 3 different enviornments (eg: home, campus, bus, or whever else you are comfortable using your mics). Make sure to use the same 3 environments with both mics.\n",
        "   \n",
        "   1.3 You should have 2 * 3 * 5 = 30 recordings, divided into 6 groups of 5 recordings.\n",
        "2. Create speaker embeddings for audio files using the mean pooling code for each group.\n",
        "3. Create your own speaker embedding cluster and add it to one of the plots above with the other speakers in the dataset. You can choose the layer of your choice. Note that you should add 6 additional groups in the existing clusters. This graph is the final deliverable (6 points).\n",
        "\n",
        "Answer the following questions:\n",
        "1. What do you observe about how the 6 groups are placed with respect to the clusters of the other speakers? Explain why. (2 points)\n",
        "2. What do you observe about how the 6 groups are arranged amongst each other? Explain why. (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2f96dd0",
      "metadata": {
        "id": "d2f96dd0"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}