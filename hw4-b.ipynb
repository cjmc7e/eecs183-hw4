{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lvqHPrzvoE5"
      },
      "source": [
        "# HW4 (b): Automatic Speech Recognition (ASR) by Fine-Tuning SSL models (30 points)\n",
        "\n",
        "In this lab, you will learn how to retrieve speech SSL models through Hugging Face, and fine-tune the model for the ASR task.\n",
        "\n",
        "In particular, we will apply Connectionist Temporal Classification (CTC; [paper](https://dl.acm.org/doi/abs/10.1145/1143844.1143891)). We provide a hands-on experience in implementing and training an ASR model. The training takes several hours and may hit the limit of GPU capacity of free version of colab, so please START EARLY!!.\n",
        "\n",
        "**Note - To overcome GPU limit, save checkpoints every epoch, so you can restart your run from that epoch.**\n",
        "\n",
        "## **About submission**\n",
        "Below, you will be asked to fill in some cells to implement the model. For submission, please make a zip containing this notebook with a complete implementation, which is the best performing model based on the validation WER. You should also use this model to generate the test-set transcriptions.\n",
        "\n",
        "For this assignment, you will need to submit transcriptions for the test-set provided with the data as a text file in the exact format of the specified audio files.\n",
        "\n",
        "The submission file should have the name: 'asr_submission.txt'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGWnBNug7mNM"
      },
      "source": [
        "## 1. Setting up environment and data\n",
        "\n",
        "Please set a GPU session.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Install to the Python environment that Jupyter is using\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\", \"flashlight-text\", \"google\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPlFbjvceUww",
        "outputId": "f852446b-a909-46b3-bcdc-d9c7c98ce584"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLU3Q2VAmDqq",
        "outputId": "7137b193-031f-43ca-da06-41f3ab0f49db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PackagesNotFoundError: The following packages are not available from current channels:\n",
            "\n",
            "  - flashlight-text\n",
            "\n",
            "Current channels:\n",
            "\n",
            "  - https://repo.anaconda.com/pkgs/main/win-64\n",
            "  - https://repo.anaconda.com/pkgs/main/noarch\n",
            "  - https://repo.anaconda.com/pkgs/r/win-64\n",
            "  - https://repo.anaconda.com/pkgs/r/noarch\n",
            "  - https://repo.anaconda.com/pkgs/msys2/win-64\n",
            "  - https://repo.anaconda.com/pkgs/msys2/noarch\n",
            "\n",
            "To search for alternate channels that may provide the conda package you're\n",
            "looking for, navigate to\n",
            "\n",
            "    https://anaconda.org\n",
            "\n",
            "and use the search bar at the top of the page.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install transformers flashlight-text\n",
        "!conda install transformers flashlight-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn2s5s7q9MgG"
      },
      "source": [
        "Download the dataset from https://drive.google.com/file/d/1KEX_sLTRGOt82DjMsOt7ItqwI7SFN49G/view?usp=sharing\n",
        "\n",
        "This dataset is a mini-subset of [LibriSpeech](https://https://ieeexplore.ieee.org/abstract/document/7178964), a most commonly used speech dataset that is composed of audiobook recordings.\n",
        "\n",
        "The mini-version created for this assignment has 10 hours of multispeaker English audios for training and one hour for validation and test set.\n",
        "\n",
        "Please place the zip file under your Google drive folder, YOUR_DIR. Then, follow the below instructions to set up the dataset.\n",
        "\n",
        "The texts are already normalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "HwD9-HbxkiyY",
        "outputId": "c09fc960-030e-464a-a173-4c858c8324be"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Mounting your Google drive to Colab environment.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Mount Google Drive\u001b[39;00m\n\u001b[0;32m      6\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "# Mounting your Google drive to Colab environment.\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al7WBWq-17au"
      },
      "outputs": [],
      "source": [
        "data_zip = '/content/drive/MyDrive/183/student_dataset.zip'\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(data_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/MyDrive/183dataset/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jY7BwjRdlckt"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"C:/Users/cjmc7/Documents/student_dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7tm-kd19jq5"
      },
      "source": [
        "## 2. Basic helper functions for dataloading and tokenizing.\n",
        "\n",
        "We use PyTorch library for training the model. Here, we provide basic helper functions to set up the dataloader, and tokenize texts to character indices.\n",
        "\n",
        "\n",
        "You don't have to implement anything here, but we highly recommend to go over each function carefully.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VFvDUrItloK_"
      },
      "outputs": [],
      "source": [
        "# Loading packages\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import tqdm\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CK3MF4skl4ms"
      },
      "outputs": [],
      "source": [
        "def load_transcription(transcription_file):\n",
        "    '''\n",
        "    Load transcription\n",
        "    '''\n",
        "    tag2text = {}\n",
        "    with open(transcription_file, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            tag, text = line.rstrip().split(\"|\")\n",
        "            tag2text[tag] = text\n",
        "    return tag2text\n",
        "\n",
        "class CharacterTokenizer(nn.Module):\n",
        "    '''\n",
        "    Tokenize texts to indices of charactors, and decode indices back to texts.\n",
        "    '''\n",
        "    def __init__(self,):\n",
        "        super().__init__()\n",
        "        self.charactors = ['blank','pad', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
        "                           'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
        "                           'U', 'V', 'W', 'X', 'Y', 'Z', ' ', \"'\" ]\n",
        "        self.ch2idx = {ch:i for i, ch in enumerate(self.charactors)}\n",
        "        self.pad_id = 1\n",
        "        self.blank =  0\n",
        "        self.vocab = self.charactors\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        token_idxs = np.array([self.ch2idx[t] for t in text.upper() if t in self.charactors])\n",
        "        return token_idxs\n",
        "\n",
        "    def enocode_torch_batch(self, texts):\n",
        "        token_idxs_batch = [torch.from_numpy(self.encode(text)).long() for text in texts]\n",
        "        token_lens = torch.Tensor([len(token_idxs) for token_idxs in token_idxs_batch]).long()\n",
        "        token_idxs_batch = nn.utils.rnn.pad_sequence(token_idxs_batch,\n",
        "                                                batch_first=True, padding_value=self.pad_id)\n",
        "        return token_idxs_batch, token_lens\n",
        "\n",
        "    def decode(self, token_idxs):\n",
        "        return ''.join([self.charactors[i] for i in token_idxs if i not in [self.pad_id, self.blank]])\n",
        "\n",
        "    def pad_id(self):\n",
        "        return self.pad_id\n",
        "\n",
        "    def blank_id(self):\n",
        "        return self.blank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7a9v0Lo_l4fP"
      },
      "outputs": [],
      "source": [
        "class SpeechTextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, DATA_DIR, split='train',):\n",
        "        super().__init__()\n",
        "        DATA_DIR = Path(DATA_DIR)\n",
        "        # Use split-specific transcription files\n",
        "        if split == 'train':\n",
        "            self.tag2text = load_transcription(DATA_DIR/\"train_transcriptions.txt\")\n",
        "        elif split == 'dev':\n",
        "            self.tag2text = load_transcription(DATA_DIR/\"dev_transcriptions.txt\")\n",
        "        else:  # test split\n",
        "            self.tag2text = {}  # No transcriptions for test set\n",
        "        self.wav_files = [f for f in (DATA_DIR/split).glob(\"*.flac\")]\n",
        "        self.wav_files.sort()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wav_files)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        wav_file = self.wav_files[i]\n",
        "        # load wave form\n",
        "        wav,sr = torchaudio.load(wav_file) # shape = (1, L) for mono-sound\n",
        "\n",
        "        # z-score wave form\n",
        "        wav = (wav-wav.mean())/wav.std()\n",
        "        assert sr ==16000\n",
        "\n",
        "        text = self.tag2text[wav_file.stem]\n",
        "\n",
        "        output = {'wav':wav[0],\n",
        "                  'text':text}\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def collate(batch):\n",
        "        data = {}\n",
        "        input_values =  nn.utils.rnn.pad_sequence([d['wav'] for d in batch],\n",
        "                                                batch_first=True, padding_value=0.0)\n",
        "        data['input_values'] = input_values\n",
        "        data['attention_mask'] = nn.utils.rnn.pad_sequence([torch.ones(len(d['wav'])) for d in batch],\n",
        "                                                batch_first=True, padding_value=0)\n",
        "        data['text'] = [d['text'] for d in batch]\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wLGcUVprl4Wn"
      },
      "outputs": [],
      "source": [
        "# Initialize dataset\n",
        "# The default mode is 'train'\n",
        "dataset = SpeechTextDataset(DATA_DIR, 'train')  # or 'dev' for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIro1ecol4ML",
        "outputId": "68dfbac2-0086-40f9-c45c-4c76120664a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'wav': tensor([ 0.0164,  0.0142,  0.0120,  ...,  0.0133, -0.0030, -0.0109]),\n",
              " 'text': 'PSYCHOTHERAPY AND THE COMMUNITY BOTH THE PHYSICIAN AND THE PATIENT FIND THEIR PLACE IN THE COMMUNITY THE LIFE INTERESTS OF WHICH ARE SUPERIOR TO THE INTERESTS OF THE INDIVIDUAL'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Check how a data point looks like.\n",
        "\n",
        "dataset.__getitem__(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2ydnEG7-X44"
      },
      "source": [
        "## 3. Implementation of CTC ASR model with a pre-trained SSL upstream\n",
        "\n",
        "Here, we will load a pre-trained Hubert-base model from huggingface. The ASR model will be constructed as an SSL model upstream and some layers of LSTMs.\n",
        "\n",
        "The model will be trained using CTC (`nn.CTCLoss`).\n",
        "\n",
        "Please fill below `YOUR CODE` sections to complete the model.\n",
        "\n",
        "You need to decide which layer in the Transformer encoder in SSL model to use (refer to probing assignment for this), and LSTM configuration to run.\n",
        "(Please don't try very large models. Colab would not allow it.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK8JMx0nl3hK"
      },
      "outputs": [],
      "source": [
        "# from torchaudio.models.decoder import ctc_decoder\n",
        "\n",
        "# ######################## YOUR CODE ######################\n",
        "# # you can find more from Hugging Face!\n",
        "# # https://huggingface.co/\n",
        "# from transformers import TODO\n",
        "# #########################################################\n",
        "\n",
        "# class ASR(nn.Module):\n",
        "#     def __init__(self,\n",
        "#                  ######################## YOUR CODE ######################\n",
        "#                  # Please add any arguments required to initiate the model\n",
        "#                  #########################################################\n",
        "#                  ):\n",
        "\n",
        "\n",
        "#         super().__init__()\n",
        "\n",
        "\n",
        "#         self.token_processor = CharacterTokenizer()\n",
        "\n",
        "#         ######################## YOUR CODE ######################\n",
        "#         self.upstream =  TODO ## Initiate and load an SSL model,\n",
        "#         # If you figure out the function call correctly,\n",
        "#         # you will be able to change some config of the SSL (e.g., layer num) when you load the model.\n",
        "\n",
        "#         self.lstm = TODO # LSTM module\n",
        "#         self.lstm_output_size = TODO\n",
        "#         self.logit = nn.Linear(self.lstm_output_size, len(self.token_processor.vocab)) # Final classification layer\n",
        "\n",
        "#         # We put a downsampling convolution here to reduce temporal resoultion from 50Hz to 25Hz\n",
        "#         # to reduce the computation load.\n",
        "#         # You need to properly set ssl_output_size, and  lstm_input_size.\n",
        "#         #########################################################\n",
        "\n",
        "#         self.downsample = nn.Conv1d(ssl_output_size, lstm_input_size,\n",
        "#                                     kernel_size=2, stride=2, padding=0, dilation=1,)\n",
        "#         self.loss = nn.CTCLoss(blank = self.token_processor.blank_id(),zero_infinity=True,reduction='mean')\n",
        "#         self.ctc_decoder = ctc_decoder(lexicon=None,\n",
        "#                                       tokens=self.token_processor.vocab,\n",
        "#                                       lm=None,\n",
        "#                                       lm_dict=None,\n",
        "#                                       nbest=1,\n",
        "#                                       beam_size=5,\n",
        "#                                       blank_token=\"blank\",\n",
        "#                                       sil_token=\" \",\n",
        "#                                      )\n",
        "\n",
        "#     def forward(self, input_values, text, attention_mask=None):\n",
        "\n",
        "#         if attention_mask is None:\n",
        "#             attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "#         text_idxs, text_lens = self.token_processor.enocode_torch_batch(text)\n",
        "#         text_idxs = text_idxs.to(input_values.device).to(torch.int32)\n",
        "#         text_lens = text_lens.to(input_values.device).to(torch.int32)\n",
        "\n",
        "#         ######################## YOUR CODE ######################\n",
        "#         # fill in the argument below to run SSL upstream model.\n",
        "#         # the resulting \"source_encodings\" should be (Batch size, Length, Dimension).\n",
        "#         source_encodings = self.upstream(TODO).last_hidden_state\n",
        "#         #########################################################\n",
        "\n",
        "#         source_encodings = self.downsample(source_encodings.transpose(1,2)).transpose(1,2) # Don't have to change\n",
        "\n",
        "#         ######################## YOUR CODE ######################\n",
        "#         # You need to implement the code for the rest of the part (LSTM, logit, ...)\n",
        "#         #\n",
        "#         # And fill in the below loss code for calling CTCLoss\n",
        "#         # Tip: check argument types/shapes in https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "#         loss = TODO\n",
        "#         #########################################################\n",
        "\n",
        "#         return loss\n",
        "\n",
        "#     def predict(self, input_values, attention_mask=None):\n",
        "\n",
        "#         if attention_mask is None:\n",
        "#             attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "#         ######################## YOUR CODE ######################\n",
        "#         # fill in the inference code. This should be the same as \"forward\" function except loss calculation.\n",
        "#         #########################################################\n",
        "\n",
        "#         pred_texts = self.ctc_decoder(source_encodings.cpu(), source_lengths.cpu())\n",
        "#         pred_texts = [self.token_processor.decode(pred_text[0].tokens) for pred_text in pred_texts]\n",
        "#         return pred_texts, source_encodings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchaudio.models.decoder import ctc_decoder\n",
        "\n",
        "######################## YOUR CODE ######################\n",
        "from transformers import HubertModel\n",
        "#########################################################\n",
        "\n",
        "class ASR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 ######################## YOUR CODE ######################\n",
        "                 ssl_layer=10,  # Based on probing results, layer 10 was best for phonemes\n",
        "                 lstm_hidden_size=256,\n",
        "                 lstm_num_layers=2,\n",
        "                 lstm_dropout=0.1,\n",
        "                 #########################################################\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_processor = CharacterTokenizer()\n",
        "\n",
        "        ######################## YOUR CODE ######################\n",
        "        # Load HuBERT model and configure to output specific layer\n",
        "        self.upstream = HubertModel.from_pretrained(\n",
        "            \"facebook/hubert-base-ls960\",\n",
        "            num_hidden_layers=ssl_layer + 1  # +1 because layer 0 is embedding\n",
        "        )\n",
        "        # Freeze upstream model to save memory and computation -- disabling this\n",
        "        # for param in self.upstream.parameters():\n",
        "        #     param.requires_grad = False\n",
        "\n",
        "        # SSL output size for HuBERT base\n",
        "        ssl_output_size = 768\n",
        "\n",
        "        # LSTM input size after downsampling\n",
        "        lstm_input_size = 768\n",
        "\n",
        "        # LSTM configuration\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=lstm_input_size,\n",
        "            hidden_size=lstm_hidden_size,\n",
        "            num_layers=lstm_num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=lstm_dropout if lstm_num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # LSTM output size (bidirectional doubles the size)\n",
        "        self.lstm_output_size = lstm_hidden_size * 2\n",
        "\n",
        "        self.logit = nn.Linear(self.lstm_output_size, len(self.token_processor.vocab))\n",
        "        #########################################################\n",
        "\n",
        "        # Downsampling convolution\n",
        "        self.downsample = nn.Conv1d(ssl_output_size, lstm_input_size,\n",
        "                                    kernel_size=2, stride=2, padding=0, dilation=1)\n",
        "\n",
        "        self.loss = nn.CTCLoss(blank=self.token_processor.blank_id(), zero_infinity=True, reduction='mean')\n",
        "        self.ctc_decoder = ctc_decoder(lexicon=None,\n",
        "                                      tokens=self.token_processor.vocab,\n",
        "                                      lm=None,\n",
        "                                      lm_dict=None,\n",
        "                                      nbest=1,\n",
        "                                      beam_size=5,\n",
        "                                      blank_token=\"blank\",\n",
        "                                      sil_token=\" \",\n",
        "                                     )\n",
        "\n",
        "    def forward(self, input_values, text, attention_mask=None):\n",
        "\n",
        "      if attention_mask is None:\n",
        "          attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "      text_idxs, text_lens = self.token_processor.enocode_torch_batch(text)\n",
        "      text_idxs = text_idxs.to(input_values.device).to(torch.int32)\n",
        "      text_lens = text_lens.to(input_values.device).to(torch.int32)\n",
        "\n",
        "      ######################## YOUR CODE ######################\n",
        "      # Run SSL upstream model\n",
        "      source_encodings = self.upstream(\n",
        "          input_values=input_values,\n",
        "          attention_mask=attention_mask\n",
        "      ).last_hidden_state\n",
        "      #########################################################\n",
        "\n",
        "      # Downsample\n",
        "      source_encodings = self.downsample(source_encodings.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "      ######################## YOUR CODE ######################\n",
        "      # LSTM forward pass\n",
        "      lstm_out, _ = self.lstm(source_encodings)\n",
        "\n",
        "      # Get logits\n",
        "      logits = self.logit(lstm_out)  # (B, T, vocab_size)\n",
        "\n",
        "      # Prepare for CTC loss\n",
        "      # CTC expects: (T, B, C) format\n",
        "      log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "      log_probs = log_probs.transpose(0, 1)  # (T, B, C)\n",
        "\n",
        "      # Calculate input lengths based on actual sequence length after processing\n",
        "      # source_encodings has shape (B, T_processed, D)\n",
        "      # The actual sequence length is T_processed (the time dimension)\n",
        "      # input_lengths = torch.full((source_encodings.size(0),), source_encodings.size(1), dtype=torch.int32, device=source_encodings.device)\n",
        "      input_lengths = (attention_mask.sum(dim=1) / 640).long()\n",
        "      input_lengths = input_lengths.clamp(max=log_probs.size(0))\n",
        "\n",
        "      # CTC Loss\n",
        "      loss = self.loss(\n",
        "          log_probs=log_probs,\n",
        "          targets=text_idxs,\n",
        "          input_lengths=input_lengths,\n",
        "          target_lengths=text_lens\n",
        "      )\n",
        "      #########################################################\n",
        "\n",
        "      return loss\n",
        "\n",
        "    def predict(self, input_values, attention_mask=None):\n",
        "\n",
        "      if attention_mask is None:\n",
        "          attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "      ######################## YOUR CODE ######################\n",
        "      # Run SSL upstream model\n",
        "      source_encodings = self.upstream(\n",
        "          input_values=input_values,\n",
        "          attention_mask=attention_mask\n",
        "      ).last_hidden_state\n",
        "\n",
        "      # Downsample\n",
        "      source_encodings = self.downsample(source_encodings.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "      # LSTM forward pass\n",
        "      lstm_out, _ = self.lstm(source_encodings)\n",
        "\n",
        "      # Get logits\n",
        "      logits = self.logit(lstm_out)  # (B, T, vocab_size)\n",
        "\n",
        "      # Calculate source lengths based on actual sequence length\n",
        "      source_lengths = torch.full((source_encodings.size(0),), source_encodings.size(1), dtype=torch.int32)\n",
        "      #########################################################\n",
        "\n",
        "      pred_texts = self.ctc_decoder(logits.cpu(), source_lengths.cpu())\n",
        "      pred_texts = [self.token_processor.decode(pred_text[0].tokens) for pred_text in pred_texts]\n",
        "\n",
        "      return pred_texts, source_encodings"
      ],
      "metadata": {
        "id": "nEGU5jHbGcoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 3\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from transformers import HubertModel, HubertConfig\n",
        "from torchaudio.models.decoder import ctc_decoder\n",
        "\n",
        "class ASR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hubert_layer=10,\n",
        "                 lstm_hidden_size=768,\n",
        "                 lstm_num_layers=2,\n",
        "                 lstm_dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_processor = CharacterTokenizer()\n",
        "\n",
        "        # Load HuBERT model with specific layer configuration\n",
        "        config = HubertConfig.from_pretrained('facebook/hubert-base-ls960')\n",
        "        config.num_hidden_layers = hubert_layer\n",
        "        self.upstream = HubertModel.from_pretrained('facebook/hubert-base-ls960', config=config)\n",
        "\n",
        "        # Freeze upstream (comment out to fine-tune)\n",
        "        for param in self.upstream.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # LSTM decoder\n",
        "        ssl_output_size = 768  # HuBERT-base hidden size\n",
        "        lstm_input_size = 768\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=lstm_input_size,\n",
        "            hidden_size=lstm_hidden_size,\n",
        "            num_layers=lstm_num_layers,\n",
        "            dropout=lstm_dropout if lstm_num_layers > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.lstm_output_size = lstm_hidden_size * 2  # Bidirectional\n",
        "        self.logit = nn.Linear(self.lstm_output_size, len(self.token_processor.vocab))\n",
        "\n",
        "        # Downsampling convolution (50Hz -> 25Hz)\n",
        "        self.downsample = nn.Conv1d(ssl_output_size, lstm_input_size,\n",
        "                                     kernel_size=2, stride=2, padding=0, dilation=1)\n",
        "\n",
        "        # CTC loss\n",
        "        self.loss = nn.CTCLoss(blank=self.token_processor.blank_id(), zero_infinity=True, reduction='mean')\n",
        "\n",
        "        # CTC decoder\n",
        "        self.ctc_decoder = ctc_decoder(lexicon=None,\n",
        "                                       tokens=self.token_processor.vocab,\n",
        "                                       lm=None,\n",
        "                                       lm_dict=None,\n",
        "                                       nbest=1,\n",
        "                                       beam_size=5,\n",
        "                                       blank_token=\"blank\",\n",
        "                                       sil_token=\" \")\n",
        "\n",
        "    def forward(self, input_values, text, attention_mask=None):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "        text_idxs, text_lens = self.token_processor.enocode_torch_batch(text)\n",
        "        text_idxs = text_idxs.to(input_values.device).to(torch.int32)\n",
        "        text_lens = text_lens.to(input_values.device).to(torch.int32)\n",
        "\n",
        "        # SSL upstream encoding\n",
        "        source_encodings = self.upstream(input_values, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "        # Downsample\n",
        "        source_encodings = self.downsample(source_encodings.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(source_encodings)\n",
        "\n",
        "        # Logits\n",
        "        logits = self.logit(lstm_out)\n",
        "\n",
        "        # Log probabilities for CTC\n",
        "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "        # Calculate source lengths\n",
        "        audio_lengths = attention_mask.sum(dim=1)\n",
        "        hubert_lengths = ((audio_lengths - 400) // 320) + 1\n",
        "        source_lengths = hubert_lengths // 2\n",
        "        source_lengths = source_lengths.to(torch.int32)\n",
        "\n",
        "        # Transpose for CTC: (T, B, C)\n",
        "        log_probs = log_probs.transpose(0, 1)\n",
        "\n",
        "        # CTC loss\n",
        "        loss = self.loss(log_probs, text_idxs, source_lengths, text_lens)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def predict(self, input_values, attention_mask=None):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "        # SSL encoding\n",
        "        source_encodings = self.upstream(input_values, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "        # Downsample\n",
        "        source_encodings = self.downsample(source_encodings.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(source_encodings)\n",
        "\n",
        "        # Logits\n",
        "        logits = self.logit(lstm_out)\n",
        "\n",
        "        # Log probabilities\n",
        "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "        # Calculate source lengths\n",
        "        audio_lengths = attention_mask.sum(dim=1)\n",
        "        hubert_lengths = ((audio_lengths - 400) // 320) + 1\n",
        "        source_lengths = hubert_lengths // 2\n",
        "        source_lengths = source_lengths.cpu().to(torch.int32)\n",
        "\n",
        "        # CTC decode\n",
        "        pred_texts = self.ctc_decoder(log_probs.cpu(), source_lengths.cpu())\n",
        "        pred_texts = [self.token_processor.decode(pred_text[0].tokens) for pred_text in pred_texts]\n",
        "\n",
        "        return pred_texts, source_encodings"
      ],
      "metadata": {
        "id": "eMif2QylcXjJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Your complete ASR class (copy this exactly)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import HubertModel, HubertConfig\n",
        "from torchaudio.models.decoder import ctc_decoder\n",
        "\n",
        "class ASR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hubert_model_path='./hubert_model',\n",
        "                 hubert_layer=10,\n",
        "                 lstm_hidden_size=768,\n",
        "                 lstm_num_layers=2,\n",
        "                 lstm_dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_processor = CharacterTokenizer()\n",
        "\n",
        "        print(f\"Loading HuBERT from {hubert_model_path}...\")\n",
        "\n",
        "        config = HubertConfig.from_pretrained(\n",
        "            hubert_model_path,\n",
        "            local_files_only=True\n",
        "        )\n",
        "        config.num_hidden_layers = hubert_layer\n",
        "\n",
        "        self.upstream = HubertModel.from_pretrained(\n",
        "            hubert_model_path,\n",
        "            config=config,\n",
        "            local_files_only=True\n",
        "        )\n",
        "\n",
        "        print(\"✓ HuBERT loaded!\")\n",
        "\n",
        "        # Freeze upstream\n",
        "        for param in self.upstream.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # LSTM decoder\n",
        "        ssl_output_size = 768\n",
        "        lstm_input_size = 768\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=lstm_input_size,\n",
        "            hidden_size=lstm_hidden_size,\n",
        "            num_layers=lstm_num_layers,\n",
        "            dropout=lstm_dropout if lstm_num_layers > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.lstm_output_size = lstm_hidden_size * 2\n",
        "        self.logit = nn.Linear(self.lstm_output_size, len(self.token_processor.vocab))\n",
        "\n",
        "        self.downsample = nn.Conv1d(ssl_output_size, lstm_input_size,\n",
        "                                     kernel_size=2, stride=2, padding=0, dilation=1)\n",
        "\n",
        "        self.loss = nn.CTCLoss(blank=self.token_processor.blank_id(),\n",
        "                               zero_infinity=True, reduction='mean')\n",
        "\n",
        "        self.ctc_decoder = ctc_decoder(lexicon=None,\n",
        "                                       tokens=self.token_processor.vocab,\n",
        "                                       lm=None,\n",
        "                                       lm_dict=None,\n",
        "                                       nbest=1,\n",
        "                                       beam_size=5,\n",
        "                                       blank_token=\"blank\",\n",
        "                                       sil_token=\" \")\n",
        "\n",
        "    def forward(self, input_values, text, attention_mask=None):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "        text_idxs, text_lens = self.token_processor.enocode_torch_batch(text)\n",
        "        text_idxs = text_idxs.to(input_values.device).to(torch.int32)\n",
        "        text_lens = text_lens.to(input_values.device).to(torch.int32)\n",
        "\n",
        "        source_encodings = self.upstream(input_values, attention_mask=attention_mask).last_hidden_state\n",
        "        source_encodings = self.downsample(source_encodings.transpose(1, 2)).transpose(1, 2)\n",
        "        lstm_out, _ = self.lstm(source_encodings)\n",
        "        logits = self.logit(lstm_out)\n",
        "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "        audio_lengths = attention_mask.sum(dim=1)\n",
        "        hubert_lengths = ((audio_lengths - 400) // 320) + 1\n",
        "        source_lengths = hubert_lengths // 2\n",
        "        source_lengths = source_lengths.to(torch.int32)\n",
        "\n",
        "        log_probs = log_probs.transpose(0, 1)\n",
        "        loss = self.loss(log_probs, text_idxs, source_lengths, text_lens)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def predict(self, input_values, attention_mask=None):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "        source_encodings = self.upstream(input_values, attention_mask=attention_mask).last_hidden_state\n",
        "        source_encodings = self.downsample(source_encodings.transpose(1, 2)).transpose(1, 2)\n",
        "        lstm_out, _ = self.lstm(source_encodings)\n",
        "        logits = self.logit(lstm_out)\n",
        "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "        audio_lengths = attention_mask.sum(dim=1)\n",
        "        hubert_lengths = ((audio_lengths - 400) // 320) + 1\n",
        "        source_lengths = hubert_lengths // 2\n",
        "        source_lengths = source_lengths.cpu().to(torch.int32)\n",
        "\n",
        "        pred_texts = self.ctc_decoder(log_probs.cpu(), source_lengths.cpu())\n",
        "        pred_texts = [self.token_processor.decode(pred_text[0].tokens) for pred_text in pred_texts]\n",
        "\n",
        "        return pred_texts, source_encodings"
      ],
      "metadata": {
        "id": "KkQNdSF6rdbi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoBd7QLfDOJs"
      },
      "source": [
        "## 4. Training\n",
        "\n",
        "Training ASR model usually takes long time and consumes a lot of GPU space. Here, we won't ask to reach convergence to get very low WER (try if you have enough capacity in Colab!).\n",
        "\n",
        "Below, we put very minimal setting for training, in which we expect to see below 50% WER when the model is trained for 20 epochs, if successfully implemented. As mentioned, the fine-tuning can get below 20% WER. Also, the loss may seem not improving for a few first epochs, which is natural for CTC. So be patient and please try early, so you can try many.\n",
        "\n",
        "This procedure takes several hours.\n",
        "\n",
        "NOTE - The choise of layer used for training ASR will be crucial. Try training the model with different layers.\n",
        "\n",
        "The points distribution based on WER on the test set:\n",
        "\n",
        "< 70% : 10 points\n",
        "\n",
        "< 50% : 15 points\n",
        "\n",
        "< 45% : 20 points\n",
        "\n",
        "< 40% : 25 points\n",
        "\n",
        "< 35% : 30 points (Full points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "g2eet5tOl2fB"
      },
      "outputs": [],
      "source": [
        "## Helper functions for WER/CER calculation and train/validation loop.\n",
        "\n",
        "def get_wer(gt_text, pred_text):\n",
        "    gt_text_tokens = [t for t in gt_text.upper().split(' ') if t != '']\n",
        "    pred_text_tokens = [t for t in pred_text.upper().split(' ') if t != '']\n",
        "    return torchaudio.functional.edit_distance(pred_text_tokens,gt_text_tokens) /len(gt_text_tokens)\n",
        "\n",
        "def get_cer(gt_text, pred_text):\n",
        "    return torchaudio.functional.edit_distance(pred_text.upper(),gt_text.upper()) /len(gt_text)\n",
        "\n",
        "\n",
        "def train_loop(dataloader, model, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    report_step = np.ceil(len(dataloader)/5).astype(int)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch_i, batch in enumerate(dataloader):\n",
        "        input_values = batch['input_values'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        text = batch['text']\n",
        "        # Compute loss\n",
        "        loss = model(input_values, text, attention_mask)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch_i % report_step == 0:\n",
        "            loss, current = loss.item(), batch_i * batch_size + len(input_values)\n",
        "            print(f\"    Train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    wer, cer = 0, 0\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_values = batch['input_values'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            gt_texts = batch['text']\n",
        "            pred_texts, _ = model.predict(input_values, attention_mask)\n",
        "            for gt_text, pred_text in zip(gt_texts, pred_texts):\n",
        "                wer += get_wer(gt_text, pred_text)\n",
        "                cer += get_cer(gt_text, pred_text)\n",
        "                cnt += 1\n",
        "\n",
        "    wer = wer/cnt\n",
        "    cer = cer/cnt\n",
        "    return wer, cer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HEpJ1I0fziae",
        "outputId": "53a8fa12-2442-4229-e79e-f8c138c14ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "# If False, please check the runtime type of the session.\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dSQ3yFwrz_iV"
      },
      "outputs": [],
      "source": [
        "## Training config\n",
        "\n",
        "device = 'cuda'\n",
        "batch_size = 4 # You may meet GPU limit if you put this number higher.\n",
        "epoch = 20 # This number is not enough for convergence but sufficient to pass the assignment.\n",
        "lr = 4e-5 # this should be sufficiently small (was originally 4e-5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Download using Python requests (works on Windows)\n",
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def download_file(url, filepath):\n",
        "    \"\"\"Download file with progress bar\"\"\"\n",
        "    print(f\"Downloading {os.path.basename(filepath)}...\")\n",
        "\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "    with open(filepath, 'wb') as file:\n",
        "        if total_size == 0:\n",
        "            file.write(response.content)\n",
        "        else:\n",
        "            downloaded = 0\n",
        "            for data in response.iter_content(chunk_size=4096):\n",
        "                downloaded += len(data)\n",
        "                file.write(data)\n",
        "                done = int(50 * downloaded / total_size)\n",
        "                print(f\"\\r[{'=' * done}{' ' * (50-done)}] {downloaded}/{total_size} bytes\", end='')\n",
        "    print()  # New line after progress\n",
        "    return filepath\n",
        "\n",
        "# Create model directory\n",
        "model_dir = \"./hubert_model\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Download files\n",
        "base_url = \"https://huggingface.co/facebook/hubert-base-ls960/resolve/main/\"\n",
        "files = {\n",
        "    \"config.json\": \"config.json\",\n",
        "    \"preprocessor_config.json\": \"preprocessor_config.json\",\n",
        "    \"pytorch_model.bin\": \"pytorch_model.bin\"\n",
        "}\n",
        "\n",
        "print(\"Downloading HuBERT model files...\\n\")\n",
        "for filename, local_name in files.items():\n",
        "    url = base_url + filename\n",
        "    filepath = os.path.join(model_dir, local_name)\n",
        "\n",
        "    try:\n",
        "        download_file(url, filepath)\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f\"✓ {filename} ({size_mb:.1f} MB)\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to download {filename}: {e}\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Download Summary:\")\n",
        "print(\"=\"*50)\n",
        "for filename in files.values():\n",
        "    filepath = os.path.join(model_dir, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f\"✓ {filename}: {size_mb:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"✗ {filename}: MISSING\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcbG6DsnrM3u",
        "outputId": "cc59705a-ef75-4666-b5bb-f4b43dffb962"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading HuBERT model files...\n",
            "\n",
            "Downloading config.json...\n",
            "\n",
            "✓ config.json (0.0 MB)\n",
            "\n",
            "Downloading preprocessor_config.json...\n",
            "[==================================================] 213/213 bytes\n",
            "✓ preprocessor_config.json (0.0 MB)\n",
            "\n",
            "Downloading pytorch_model.bin...\n",
            "[==================================================] 377569754/377569754 bytes\n",
            "✓ pytorch_model.bin (360.1 MB)\n",
            "\n",
            "\n",
            "==================================================\n",
            "Download Summary:\n",
            "==================================================\n",
            "✓ config.json: 0.0 MB\n",
            "✓ preprocessor_config.json: 0.0 MB\n",
            "✓ pytorch_model.bin: 360.1 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Verify downloads and load model\n",
        "import os\n",
        "from transformers import HubertModel, HubertConfig\n",
        "\n",
        "model_dir = \"./hubert_model\"\n",
        "\n",
        "# Check all files exist\n",
        "required_files = [\"config.json\", \"preprocessor_config.json\", \"pytorch_model.bin\"]\n",
        "all_present = all(os.path.exists(os.path.join(model_dir, f)) for f in required_files)\n",
        "\n",
        "if not all_present:\n",
        "    print(\"❌ Not all files downloaded. Please run Cell 1 again.\")\n",
        "else:\n",
        "    print(\"✓ All files present. Loading model...\\n\")\n",
        "\n",
        "    # Load configuration\n",
        "    config = HubertConfig.from_pretrained(\n",
        "        model_dir,\n",
        "        local_files_only=True\n",
        "    )\n",
        "    config.num_hidden_layers = 10\n",
        "\n",
        "    # Load model\n",
        "    hubert = HubertModel.from_pretrained(\n",
        "        model_dir,\n",
        "        config=config,\n",
        "        local_files_only=True\n",
        "    )\n",
        "\n",
        "    print(\"✓ HuBERT loaded successfully!\")\n",
        "    print(f\"  - Model layers: {config.num_hidden_layers}\")\n",
        "    print(f\"  - Hidden size: {config.hidden_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4heiIzmrYZf",
        "outputId": "05ec1c81-97b7-49c2-cf0f-bb3ab628ee5c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All files present. Loading model...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\cjmc7\\anaconda3\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "Some weights of the model checkpoint at ./hubert_model were not used when initializing HubertModel: ['encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.10.attention.k_proj.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.attention.k_proj.weight']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ HuBERT loaded successfully!\n",
            "  - Model layers: 10\n",
            "  - Hidden size: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lTKY4ou7zAYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c19017-4440-456b-f53b-ed5570bef40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading HuBERT from ./hubert_model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./hubert_model were not used when initializing HubertModel: ['encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.10.attention.k_proj.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.attention.k_proj.weight']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ HuBERT loaded!\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "model = ASR()\n",
        "model = model.to(device).train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lTLonDcSzJX-"
      },
      "outputs": [],
      "source": [
        "train_dataset = SpeechTextDataset(DATA_DIR, 'train')\n",
        "val_dataset = SpeechTextDataset(DATA_DIR, 'dev')\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=0, # Changed from 2 to 0 to prevent pickling issues\n",
        "                        drop_last=True,\n",
        "                        pin_memory=True,\n",
        "                        collate_fn=SpeechTextDataset.collate)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=0, # Changed from 2 to 0 to prevent pickling issues\n",
        "                        drop_last=False,\n",
        "                        pin_memory=True,\n",
        "                        collate_fn=SpeechTextDataset.collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sai1HEuQzreu"
      },
      "outputs": [],
      "source": [
        "# Set up optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def train_loop(dataloader, model, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    report_step = max(1, len(dataloader) // 5)  # Report 5 times per epoch\n",
        "    model.train()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_i, batch in enumerate(dataloader):\n",
        "        batch_start = time.time()\n",
        "\n",
        "        input_values = batch['input_values'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        text = batch['text']\n",
        "\n",
        "        # Print first batch info\n",
        "        if batch_i == 0:\n",
        "            print(f\"    First batch shape: {input_values.shape}\")\n",
        "            print(f\"    Device: {input_values.device}\")\n",
        "\n",
        "        # Compute loss\n",
        "        loss = model(input_values, text, attention_mask)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_time = time.time() - batch_start\n",
        "\n",
        "        if batch_i % report_step == 0 or batch_i == 0:\n",
        "            loss_val = loss.item()\n",
        "            current = batch_i * len(input_values)\n",
        "            print(f\"    Batch {batch_i}/{len(dataloader)} - Loss: {loss_val:>7f} - Time: {batch_time:.2f}s - [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"    Epoch completed in {total_time:.1f}s ({total_time/60:.1f}min)\")"
      ],
      "metadata": {
        "id": "N89MYAEmwDBT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq-jAbGa0FC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746ccea9-6adc-4b33-e751-89cba9fbe050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - 2025-11-14 02:54:10.992956\n",
            "    First batch shape: torch.Size([8, 236320])\n",
            "    Device: cuda:0\n",
            "    HuBERT (upstream) time: 612948.62 ms\n",
            "    Downsample time: 46.82 ms\n",
            "    LSTM time: 207.89 ms\n",
            "    Logit time: 1.25 ms\n",
            "    Batch 0/358 - Loss: 4.511792 - Time: 631.10s - [    0/ 2866]\n"
          ]
        }
      ],
      "source": [
        "best_weights = model.state_dict()\n",
        "best_wer = 99999\n",
        "for e in range(epoch):\n",
        "    print(f\"Epoch [{e+1}/{epoch}] - {datetime.now()}\")\n",
        "    train_loop(train_dataloader, model, optimizer, e)\n",
        "    val_wer, val_cer = test_loop(val_dataloader, model)\n",
        "    print(f\"    Validation WER: {(100*val_wer):>0.2f}%, CER: {(100*val_cer):>0.2f}%\")\n",
        "    if val_wer < best_wer:\n",
        "        best_weights = model.state_dict()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpyrJr2O2iJ3"
      },
      "outputs": [],
      "source": [
        "# save model checkpoint\n",
        "\n",
        "torch.save({'state_dict':model.state_dict()}, \"best_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O_011aS6jgD"
      },
      "outputs": [],
      "source": [
        "# Load your best model\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", map_location=\"cpu\")['state_dict'])\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = SpeechTextDataset(DATA_DIR, 'test')\n",
        "submission_order_file = Path(DATA_DIR) / \"test_submission_order.txt\"\n",
        "\n",
        "# Generate test transcriptions\n",
        "model.eval()\n",
        "test_transcriptions = {}\n",
        "\n",
        "print(\"Generating test transcriptions...\")\n",
        "with torch.no_grad():\n",
        "    for i in tqdm.tqdm(range(len(test_dataset)), desc=\"Transcribing test set\"):\n",
        "        sample = test_dataset[i]\n",
        "        wav_file = test_dataset.wav_files[i]\n",
        "        filename = wav_file.stem\n",
        "\n",
        "        input_values = sample['wav'].unsqueeze(0).to(device)\n",
        "        attention_mask = torch.ones_like(input_values).to(device)\n",
        "\n",
        "        pred_texts, _ = model.predict(input_values, attention_mask)\n",
        "        prediction = pred_texts[0] if len(pred_texts) > 0 else \"\"\n",
        "        test_transcriptions[filename] = prediction\n",
        "\n",
        "# Create submission file in correct order\n",
        "with open(submission_order_file, 'r') as f:\n",
        "    submission_order = [line.strip() for line in f.readlines()]\n",
        "\n",
        "output_lines = []\n",
        "for audio_id in submission_order:\n",
        "    if audio_id in test_transcriptions:\n",
        "        output_lines.append(f\"{audio_id}|{test_transcriptions[audio_id]}\")\n",
        "    else:\n",
        "        output_lines.append(f\"{audio_id}|\")\n",
        "\n",
        "with open(\"asr_submission.txt\", \"w\") as f:\n",
        "    for line in output_lines:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "print(\"Test transcriptions saved to asr_submission.txt\")\n",
        "\n",
        "### !!!! Please make sure that the output file format is what's expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfbae86b"
      },
      "source": [
        "# Task\n",
        "Optimize the performance of the ASR model's training process by reducing the complexity of the LSTM decoder and potentially increasing the batch size. Profile the `ASR` module's `forward` pass to identify bottlenecks, monitor local GPU usage, and then analyze the impact of these changes on training speed and resource utilization to confirm if the `ASR` module was the bottleneck and how it was alleviated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0d2378d"
      },
      "source": [
        "## Gather Local GPU Information\n",
        "\n",
        "### Subtask:\n",
        "Share the specifications of your local GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0eeab7e"
      },
      "source": [
        "**Reasoning**:\n",
        "To gather the specifications of the GPU available in the current Colab environment, I will execute a shell command to retrieve the GPU name and the total VRAM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69f4178a",
        "outputId": "4d659804-ad08-4b7a-f3a4-3ec73d5073ef"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_total_bytes = torch.cuda.get_device_properties(0).total_memory\n",
        "    vram_total_gb = vram_total_bytes / (1024**3)\n",
        "    print(f\"GPU Manufacturer and Model: {gpu_name}\")\n",
        "    print(f\"Total VRAM: {vram_total_gb:.2f} GB\")\n",
        "else:\n",
        "    print(\"No GPU available.\")\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Manufacturer and Model: NVIDIA GeForce RTX 2070 SUPER\n",
            "Total VRAM: 8.00 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddbba7cb"
      },
      "source": [
        "## Add Profiling to ASR Forward Pass\n",
        "\n",
        "### Subtask:\n",
        "Insert `torch.cuda.Event` calls within the `ASR` model's `forward` method to measure the execution time of different components like the `upstream` HuBERT model, `downsample` convolution, `lstm` layers, and `logit` calculation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "557b4515"
      },
      "source": [
        "**Reasoning**:\n",
        "To measure the execution time of different components within the `ASR` model's `forward` pass, I need to modify the `ASR` class definition from cell `KkQNdSF6rdbi` to include `torch.cuda.Event` calls around each targeted operation. This will allow profiling the `upstream` HuBERT model, `downsample` convolution, `lstm` layers, and `logit` calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41050e66"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import HubertModel, HubertConfig\n",
        "from torchaudio.models.decoder import ctc_decoder\n",
        "\n",
        "class ASR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hubert_model_path='./hubert_model',\n",
        "                 hubert_layer=10,\n",
        "                 lstm_hidden_size=768,\n",
        "                 lstm_num_layers=2,\n",
        "                 lstm_dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_processor = CharacterTokenizer()\n",
        "\n",
        "        print(f\"Loading HuBERT from {hubert_model_path}...\")\n",
        "\n",
        "        config = HubertConfig.from_pretrained(\n",
        "            hubert_model_path,\n",
        "            local_files_only=True\n",
        "        )\n",
        "        config.num_hidden_layers = hubert_layer\n",
        "\n",
        "        self.upstream = HubertModel.from_pretrained(\n",
        "            hubert_model_path,\n",
        "            config=config,\n",
        "            local_files_only=True\n",
        "        )\n",
        "\n",
        "        print(\"✓ HuBERT loaded!\")\n",
        "\n",
        "        # Freeze upstream\n",
        "        for param in self.upstream.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # LSTM decoder\n",
        "        ssl_output_size = 768\n",
        "        lstm_input_size = 768\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=lstm_input_size,\n",
        "            hidden_size=lstm_hidden_size,\n",
        "            num_layers=lstm_num_layers,\n",
        "            dropout=lstm_dropout if lstm_num_layers > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.lstm_output_size = lstm_hidden_size * 2\n",
        "        self.logit = nn.Linear(self.lstm_output_size, len(self.token_processor.vocab))\n",
        "\n",
        "        self.downsample = nn.Conv1d(ssl_output_size, lstm_input_size,\n",
        "                                     kernel_size=2, stride=2, padding=0, dilation=1)\n",
        "\n",
        "        self.loss = nn.CTCLoss(blank=self.token_processor.blank_id(),\n",
        "                               zero_infinity=True, reduction='mean')\n",
        "\n",
        "        self.ctc_decoder = ctc_decoder(lexicon=None,\n",
        "                                       tokens=self.token_processor.vocab,\n",
        "                                       lm=None,\n",
        "                                       lm_dict=None,\n",
        "                                       nbest=1,\n",
        "                                       beam_size=5,\n",
        "                                       blank_token=\"blank\",\n",
        "                                       sil_token=\" \")\n",
        "\n",
        "    def forward(self, input_values, text, attention_mask=None, profile=False):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "        text_idxs, text_lens = self.token_processor.enocode_torch_batch(text)\n",
        "        text_idxs = text_idxs.to(input_values.device).to(torch.int32)\n",
        "        text_lens = text_lens.to(input_values.device).to(torch.int32)\n",
        "\n",
        "        # Initialize events for profiling if requested\n",
        "        if profile and torch.cuda.is_available():\n",
        "            start_hubert = torch.cuda.Event(enable_timing=True)\n",
        "            end_hubert = torch.cuda.Event(enable_timing=True)\n",
        "            start_downsample = torch.cuda.Event(enable_timing=True)\n",
        "            end_downsample = torch.cuda.Event(enable_timing=True)\n",
        "            start_lstm = torch.cuda.Event(enable_timing=True)\n",
        "            end_lstm = torch.cuda.Event(enable_timing=True)\n",
        "            start_logit = torch.cuda.Event(enable_timing=True)\n",
        "            end_logit = torch.cuda.Event(enable_timing=True)\n",
        "        else:\n",
        "            profile = False # Disable profiling if CUDA not available\n",
        "\n",
        "        # SSL upstream encoding\n",
        "        if profile: start_hubert.record()\n",
        "        source_encodings = self.upstream(input_values, attention_mask=attention_mask).last_hidden_state\n",
        "        if profile:\n",
        "            end_hubert.record()\n",
        "            torch.cuda.synchronize()\n",
        "            print(f\"    HuBERT (upstream) time: {start_hubert.elapsed_time(end_hubert):.2f} ms\")\n",
        "\n",
        "        # Downsample\n",
        "        if profile: start_downsample.record()\n",
        "        source_encodings = self.downsample(source_encodings.transpose(1, 2)).transpose(1, 2)\n",
        "        if profile:\n",
        "            end_downsample.record()\n",
        "            torch.cuda.synchronize()\n",
        "            print(f\"    Downsample time: {start_downsample.elapsed_time(end_downsample):.2f} ms\")\n",
        "\n",
        "        # LSTM\n",
        "        if profile: start_lstm.record()\n",
        "        lstm_out, _ = self.lstm(source_encodings)\n",
        "        if profile:\n",
        "            end_lstm.record()\n",
        "            torch.cuda.synchronize()\n",
        "            print(f\"    LSTM time: {start_lstm.elapsed_time(end_lstm):.2f} ms\")\n",
        "\n",
        "        # Logits\n",
        "        if profile: start_logit.record()\n",
        "        logits = self.logit(lstm_out)\n",
        "        if profile:\n",
        "            end_logit.record()\n",
        "            torch.cuda.synchronize()\n",
        "            print(f\"    Logit time: {start_logit.elapsed_time(end_logit):.2f} ms\")\n",
        "\n",
        "        # Log probabilities for CTC\n",
        "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "        # Calculate source lengths\n",
        "        audio_lengths = attention_mask.sum(dim=1)\n",
        "        hubert_lengths = ((audio_lengths - 400) // 320) + 1\n",
        "        source_lengths = hubert_lengths // 2\n",
        "        source_lengths = source_lengths.to(torch.int32)\n",
        "\n",
        "        # Transpose for CTC: (T, B, C)\n",
        "        log_probs = log_probs.transpose(0, 1)\n",
        "\n",
        "        # CTC loss\n",
        "        loss = self.loss(log_probs, text_idxs, source_lengths, text_lens)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def predict(self, input_values, attention_mask=None):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "        # SSL encoding\n",
        "        source_encodings = self.upstream(input_values, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "        # Downsample\n",
        "        source_encodings = self.downsample(source_encodings.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(source_encodings)\n",
        "\n",
        "        # Logits\n",
        "        logits = self.logit(lstm_out)\n",
        "\n",
        "        # Log probabilities\n",
        "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "        # Calculate source lengths\n",
        "        audio_lengths = attention_mask.sum(dim=1)\n",
        "        hubert_lengths = ((audio_lengths - 400) // 320) + 1\n",
        "        source_lengths = hubert_lengths // 2\n",
        "        source_lengths = source_lengths.cpu().to(torch.int32)\n",
        "\n",
        "        # CTC decode\n",
        "        pred_texts = self.ctc_decoder(log_probs.cpu(), source_lengths.cpu())\n",
        "        pred_texts = [self.token_processor.decode(pred_text[0].tokens) for pred_text in pred_texts]\n",
        "\n",
        "        return pred_texts, source_encodings\n",
        "\n",
        "# Note: This code block replaces the previous ASR class definition.\n",
        "# You will need to re-initialize your model after running this cell."
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b64bb24"
      },
      "source": [
        "## Reduce LSTM Complexity\n",
        "\n",
        "### Subtask:\n",
        "Reduce the complexity of the LSTM decoder by modifying its hidden size and number of layers, then re-initialize the model and optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0615e83e"
      },
      "source": [
        "**Reasoning**:\n",
        "To reduce LSTM complexity, I will re-instantiate the ASR model with a smaller hidden size (256) and fewer layers (1) for the LSTM, then move it to the GPU and re-initialize the optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82972e1d",
        "outputId": "1679bbbc-7970-4f64-bec9-8ff4aa1fb4c1"
      },
      "source": [
        "model = ASR(lstm_hidden_size=256, lstm_num_layers=1)\n",
        "model = model.to(device).train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "print(\"ASR model re-initialized with reduced LSTM complexity (hidden_size=256, num_layers=1).\")\n",
        "print(f\"New optimizer created for the re-initialized model.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading HuBERT from ./hubert_model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\cjmc7\\anaconda3\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "Some weights of the model checkpoint at ./hubert_model were not used when initializing HubertModel: ['encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.10.attention.k_proj.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.attention.k_proj.weight']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ HuBERT loaded!\n",
            "ASR model re-initialized with reduced LSTM complexity (hidden_size=256, num_layers=1).\n",
            "New optimizer created for the re-initialized model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4039adcd"
      },
      "source": [
        "**Reasoning**:\n",
        "The user provided a new, more detailed subtask description for cell `2b64bb24`. I need to update the content of this `text_cell` accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "7bc09b7c",
        "outputId": "d0422aa4-6e8e-4ee3-d240-eb27c246cb23"
      },
      "source": [
        "## Reduce LSTM Complexity\n",
        "\n",
        "### Subtask:\n",
        "Reduce the complexity of the LSTM decoder by modifying its hidden size and number of layers, then re-initialize the model and optimizer.\n",
        "\n",
        "#### Instructions\n",
        "1. Modify the `ASR` class initialization by changing the `lstm_hidden_size` parameter from `768` to a smaller value (e.g., `512` or `256`).\n",
        "2. Optionally, change the `lstm_num_layers` parameter from `2` to `1` to further reduce complexity.\n",
        "3. Instantiate the `ASR` model with these new parameters (e.g., `model = ASR(lstm_hidden_size=256, lstm_num_layers=1)`).\n",
        "4. Move the `model` to the appropriate device (e.g., `model = model.to(device).train()`).\n",
        "5. Re-initialize the optimizer with the new model parameters (e.g., `optimizer = torch.optim.Adam(model.parameters(), lr=lr)`)."
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (4039499260.py, line 4)",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[28], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    Reduce the complexity of the LSTM decoder by modifying its hidden size and number of layers, then re-initialize the model and optimizer.\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58066e42"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to update cell `7bc09b7c` failed with a `SyntaxError` because the `cell_type` was incorrectly set to `python` while the content was markdown. I need to correct the `cell_type` to `markdown` to properly update the text cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "294980fd"
      },
      "source": [
        "# Task\n",
        "**Consider Increasing Batch Size**: Update the `batch_size` variable from 4 to 8, then re-initialize `train_dataloader` and `val_dataloader` with the new batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc0372e2"
      },
      "source": [
        "## Consider Increasing Batch Size\n",
        "\n",
        "### Subtask:\n",
        "Update the `batch_size` variable from 4 to 8, then re-initialize `train_dataloader` and `val_dataloader` with the new batch size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e908bff5"
      },
      "source": [
        "## Consider Increasing Batch Size\n",
        "\n",
        "### Subtask:\n",
        "Update the `batch_size` variable from 4 to 8, then re-initialize `train_dataloader` and `val_dataloader` with the new batch size.\n",
        "\n",
        "#### Instructions\n",
        "1. Update the `batch_size` variable to 8.\n",
        "2. Re-initialize the `train_dataloader` and `val_dataloader` using the updated `batch_size`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97c65c4e"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to add instructions for increasing the batch size and re-initializing dataloaders. I need to update the `batch_size` variable and then re-initialize the `train_dataloader` and `val_dataloader` as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72ca7619",
        "outputId": "c1039076-899b-4e9a-cc1d-457626c4b0e9"
      },
      "source": [
        "batch_size = 8 # Update batch_size to 8\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=0,\n",
        "                        drop_last=True,\n",
        "                        pin_memory=True,\n",
        "                        collate_fn=SpeechTextDataset.collate)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=0,\n",
        "                        drop_last=False,\n",
        "                        pin_memory=True,\n",
        "                        collate_fn=SpeechTextDataset.collate)\n",
        "\n",
        "print(f\"Batch size updated to {batch_size}.\")\n",
        "print(\"Train and validation dataloaders re-initialized with the new batch size.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size updated to 8.\n",
            "Train and validation dataloaders re-initialized with the new batch size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfe52787"
      },
      "source": [
        "## Execute Training with Profiling\n",
        "\n",
        "### Subtask:\n",
        "Run the training loop again, ensuring the 'ASR' model's 'forward' method is called with 'profile=True' for the first batch to get detailed timing information for different components, and monitor local GPU usage during this process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcf8a825"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to modify the `train_loop` function to enable profiling for the first batch of the first epoch only. I need to update the `train_loop` function definition to include a mechanism to pass `profile=True` for the first batch and `profile=False` otherwise. Additionally, the function needs to accept an `epoch_num` parameter to know if it's the first epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af883e99"
      },
      "source": [
        "import time\n",
        "def train_loop(dataloader, model, optimizer, epoch_num):\n",
        "    size = len(dataloader.dataset)\n",
        "    report_step = max(1, len(dataloader) // 5)  # Report 5 times per epoch\n",
        "    model.train()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_i, batch in enumerate(dataloader):\n",
        "        batch_start = time.time()\n",
        "\n",
        "        input_values = batch['input_values'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        text = batch['text']\n",
        "\n",
        "        # Enable profiling for the first batch of the first epoch\n",
        "        profile_batch = (epoch_num == 0 and batch_i == 0)\n",
        "\n",
        "        # Print first batch info\n",
        "        if batch_i == 0:\n",
        "            print(f\"    First batch shape: {input_values.shape}\")\n",
        "            print(f\"    Device: {input_values.device}\")\n",
        "\n",
        "        # Compute loss\n",
        "        loss = model(input_values, text, attention_mask, profile=profile_batch)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_time = time.time() - batch_start\n",
        "\n",
        "        if batch_i % report_step == 0 or batch_i == 0:\n",
        "            loss_val = loss.item()\n",
        "            current = batch_i * len(input_values)\n",
        "            print(f\"    Batch {batch_i}/{len(dataloader)} - Loss: {loss_val:>7f} - Time: {batch_time:.2f}s - [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"    Epoch completed in {total_time:.1f}s ({total_time/60:.1f}min)\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67640ac3"
      },
      "source": [
        "## Execute Training with Profiling\n",
        "\n",
        "### Subtask:\n",
        "Run the training loop again, ensuring the 'ASR' model's 'forward' method is called with 'profile=True' for the first batch to get detailed timing information for different components, and monitor local GPU usage during this process.\n",
        "\n",
        "#### Instructions\n",
        "1. Modify the `train_loop` function to call `model.forward(input_values, text, attention_mask, profile=True)` for the first batch of the first epoch, and `profile=False` for subsequent batches. This will print profiling information for the first batch.\n",
        "2. Start the training process by executing the training loop. While the training is running, use your system's GPU monitoring tools (e.g., `nvidia-smi` for NVIDIA GPUs in a separate terminal) to observe GPU utilization and memory usage. This will help understand if the GPU is being fully utilized with the increased batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ba9f525"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to update a `text_cell` with new content. I need to generate an `addTextCellAction` for the specified cell ID with the new markdown content.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}