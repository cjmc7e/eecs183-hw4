{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lvqHPrzvoE5"
      },
      "source": [
        "# HW4 (b): Automatic Speech Recognition (ASR) by Fine-Tuning SSL models (30 points)\n",
        "\n",
        "In this lab, you will learn how to retrieve speech SSL models through Hugging Face, and fine-tune the model for the ASR task.\n",
        "\n",
        "In particular, we will apply Connectionist Temporal Classification (CTC; [paper](https://dl.acm.org/doi/abs/10.1145/1143844.1143891)). We provide a hands-on experience in implementing and training an ASR model. The training takes several hours and may hit the limit of GPU capacity of free version of colab, so please START EARLY!!.\n",
        "\n",
        "**Note - To overcome GPU limit, save checkpoints every epoch, so you can restart your run from that epoch.**\n",
        "\n",
        "## **About submission**\n",
        "Below, you will be asked to fill in some cells to implement the model. For submission, please make a zip containing this notebook with a complete implementation, and the model checkpoint, `best_model.pt`, which is the best performing model based on the validation WER. You should also use this model to generate the test-set transcriptions.\n",
        "\n",
        "For this assignment, you will need to submit transcriptions for the test-set provided with the data as a text file in the exact format of the specified audio files. \n",
        "\n",
        "The submission file should have the name: 'asr_submission.txt'. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGWnBNug7mNM"
      },
      "source": [
        "## 1. Setting up environment and data\n",
        "\n",
        "Please set a GPU session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLU3Q2VAmDqq",
        "outputId": "3bdadd44-38af-4f56-f8c4-720435fe8446"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers flashlight-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn2s5s7q9MgG"
      },
      "source": [
        "Download the dataset from https://drive.google.com/file/d/1KEX_sLTRGOt82DjMsOt7ItqwI7SFN49G/view?usp=sharing\n",
        "\n",
        "This dataset is a mini-subset of [LibriSpeech](https://https://ieeexplore.ieee.org/abstract/document/7178964), a most commonly used speech dataset that is composed of audiobook recordings.\n",
        "\n",
        "The mini-version created for this assignment has 10 hours of multispeaker English audios for training and one hour for validation and test set.\n",
        "\n",
        "Please place the zip file under your Google drive folder, YOUR_DIR. Then, follow the below instructions to set up the dataset.\n",
        "\n",
        "The texts are already normalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwD9-HbxkiyY",
        "outputId": "3a8b4706-7b43-48d8-99ea-16ac7c462973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting your Google drive to Colab environment.\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_zip = '/content/drive/MyDrive/183dataset/student_dataset.zip'\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(data_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/MyDrive/183dataset/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY7BwjRdlckt"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/YOUR_DIR/DATA-FOLDER-NAME\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7tm-kd19jq5"
      },
      "source": [
        "## 2. Basic helper functions for dataloading and tokenizing.\n",
        "\n",
        "We use PyTorch library for training the model. Here, we provide basic helper functions to set up the dataloader, and tokenize texts to character indices.\n",
        "\n",
        "\n",
        "You don't have to implement anything here, but we highly recommend to go over each function carefully.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFvDUrItloK_"
      },
      "outputs": [],
      "source": [
        "# Loading packages\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import tqdm\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK3MF4skl4ms"
      },
      "outputs": [],
      "source": [
        "def load_transcription(transcription_file):\n",
        "    '''\n",
        "    Load transcription\n",
        "    '''\n",
        "    tag2text = {}\n",
        "    with open(transcription_file, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            tag, text = line.rstrip().split(\"|\")\n",
        "            tag2text[tag] = text\n",
        "    return tag2text\n",
        "\n",
        "class CharacterTokenizer(nn.Module):\n",
        "    '''\n",
        "    Tokenize texts to indices of charactors, and decode indices back to texts.\n",
        "    '''\n",
        "    def __init__(self,):\n",
        "        super().__init__()\n",
        "        self.charactors = ['blank','pad', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
        "                           'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
        "                           'U', 'V', 'W', 'X', 'Y', 'Z', ' ', \"'\" ]\n",
        "        self.ch2idx = {ch:i for i, ch in enumerate(self.charactors)}\n",
        "        self.pad_id = 1\n",
        "        self.blank =  0\n",
        "        self.vocab = self.charactors\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        token_idxs = np.array([self.ch2idx[t] for t in text.upper() if t in self.charactors])\n",
        "        return token_idxs\n",
        "\n",
        "    def enocode_torch_batch(self, texts):\n",
        "        token_idxs_batch = [torch.from_numpy(self.encode(text)).long() for text in texts]\n",
        "        token_lens = torch.Tensor([len(token_idxs) for token_idxs in token_idxs_batch]).long()\n",
        "        token_idxs_batch = nn.utils.rnn.pad_sequence(token_idxs_batch,\n",
        "                                                batch_first=True, padding_value=self.pad_id)\n",
        "        return token_idxs_batch, token_lens\n",
        "\n",
        "    def decode(self, token_idxs):\n",
        "        return ''.join([self.charactors[i] for i in token_idxs if i not in [self.pad_id, self.blank]])\n",
        "\n",
        "    def pad_id(self):\n",
        "        return self.pad_id\n",
        "\n",
        "    def blank_id(self):\n",
        "        return self.blank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a9v0Lo_l4fP"
      },
      "outputs": [],
      "source": [
        "class SpeechTextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, DATA_DIR, split='train',):\n",
        "        super().__init__()\n",
        "        DATA_DIR = Path(DATA_DIR)\n",
        "        # Use split-specific transcription files\n",
        "        if split == 'train':\n",
        "            self.tag2text = load_transcription(DATA_DIR/\"train_transcriptions.txt\")\n",
        "        elif split == 'dev':\n",
        "            self.tag2text = load_transcription(DATA_DIR/\"dev_transcriptions.txt\")\n",
        "        else:  # test split\n",
        "            self.tag2text = {}  # No transcriptions for test set\n",
        "        self.wav_files = [f for f in (DATA_DIR/split).glob(\"*.flac\")]\n",
        "        self.wav_files.sort()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wav_files)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        wav_file = self.wav_files[i]\n",
        "        # load wave form\n",
        "        wav,sr = torchaudio.load(wav_file) # shape = (1, L) for mono-sound\n",
        "\n",
        "        # z-score wave form\n",
        "        wav = (wav-wav.mean())/wav.std()\n",
        "        assert sr ==16000\n",
        "\n",
        "        text = self.tag2text[wav_file.stem]\n",
        "\n",
        "        output = {'wav':wav[0],\n",
        "                  'text':text}\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def collate(batch):\n",
        "        data = {}\n",
        "        input_values =  nn.utils.rnn.pad_sequence([d['wav'] for d in batch],\n",
        "                                                batch_first=True, padding_value=0.0)\n",
        "        data['input_values'] = input_values\n",
        "        data['attention_mask'] = nn.utils.rnn.pad_sequence([torch.ones(len(d['wav'])) for d in batch],\n",
        "                                                batch_first=True, padding_value=0)\n",
        "        data['text'] = [d['text'] for d in batch]\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLGcUVprl4Wn"
      },
      "outputs": [],
      "source": [
        "# Initialize dataset\n",
        "# The default mode is 'train'\n",
        "dataset = SpeechTextDataset(DATA_DIR, 'train')  # or 'dev' for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIro1ecol4ML",
        "outputId": "9ec41372-3dd6-4a38-d806-2ea1d16b3638"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'wav': tensor([ 0.0164,  0.0142,  0.0120,  ...,  0.0133, -0.0030, -0.0109]),\n",
              " 'text': 'PSYCHOTHERAPY AND THE COMMUNITY BOTH THE PHYSICIAN AND THE PATIENT FIND THEIR PLACE IN THE COMMUNITY THE LIFE INTERESTS OF WHICH ARE SUPERIOR TO THE INTERESTS OF THE INDIVIDUAL'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check how a data point looks like.\n",
        "\n",
        "dataset.__getitem__(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2ydnEG7-X44"
      },
      "source": [
        "## 3. Implementation of CTC ASR model with a pre-trained SSL upstream\n",
        "\n",
        "Here, we will load a pre-trained Hubert-base model from huggingface. The ASR model will be constructed as an SSL model upstream and some layers of LSTMs.\n",
        "\n",
        "The model will be trained using CTC (`nn.CTCLoss`).\n",
        "\n",
        "Please fill below `YOUR CODE` sections to complete the model.\n",
        "\n",
        "You need to decide which layer in the Transformer encoder in SSL model to use (refer to probing assignment for this), and LSTM configuration to run.\n",
        "(Please don't try very large models. Colab would not allow it.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK8JMx0nl3hK"
      },
      "outputs": [],
      "source": [
        "from torchaudio.models.decoder import ctc_decoder\n",
        "\n",
        "######################## YOUR CODE ######################\n",
        "# you can find more from Hugging Face!\n",
        "# https://huggingface.co/\n",
        "from transformers import TODO\n",
        "#########################################################\n",
        "\n",
        "class ASR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 ######################## YOUR CODE ######################\n",
        "                 # Please add any arguments required to initiate the model\n",
        "                 #########################################################\n",
        "                 ):\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.token_processor = CharacterTokenizer()\n",
        "\n",
        "        ######################## YOUR CODE ######################\n",
        "        self.upstream =  TODO ## Initiate and load an SSL model,\n",
        "        # If you figure out the function call correctly,\n",
        "        # you will be able to change some config of the SSL (e.g., layer num) when you load the model.\n",
        "\n",
        "        self.lstm = TODO # LSTM module\n",
        "        self.lstm_output_size = TODO\n",
        "        self.logit = nn.Linear(self.lstm_output_size, len(self.token_processor.vocab)) # Final classification layer\n",
        "\n",
        "        # We put a downsampling convolution here to reduce temporal resoultion from 50Hz to 25Hz\n",
        "        # to reduce the computation load.\n",
        "        # You need to properly set ssl_output_size, and  lstm_input_size.\n",
        "        #########################################################\n",
        "\n",
        "        self.downsample = nn.Conv1d(ssl_output_size, lstm_input_size,\n",
        "                                    kernel_size=2, stride=2, padding=0, dilation=1,)\n",
        "        self.loss = nn.CTCLoss(blank = self.token_processor.blank_id(),zero_infinity=True,reduction='mean')\n",
        "        self.ctc_decoder = ctc_decoder(lexicon=None,\n",
        "                                      tokens=self.token_processor.vocab,\n",
        "                                      lm=None,\n",
        "                                      lm_dict=None,\n",
        "                                      nbest=1,\n",
        "                                      beam_size=5,\n",
        "                                      blank_token=\"blank\",\n",
        "                                      sil_token=\" \",\n",
        "                                     )\n",
        "\n",
        "    def forward(self, input_values, text, attention_mask=None):\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "        text_idxs, text_lens = self.token_processor.enocode_torch_batch(text)\n",
        "        text_idxs = text_idxs.to(input_values.device).to(torch.int32)\n",
        "        text_lens = text_lens.to(input_values.device).to(torch.int32)\n",
        "\n",
        "        ######################## YOUR CODE ######################\n",
        "        # fill in the argument below to run SSL upstream model.\n",
        "        # the resulting \"source_encodings\" should be (Batch size, Length, Dimension).\n",
        "        source_encodings = self.upstream(TODO).last_hidden_state\n",
        "        #########################################################\n",
        "\n",
        "        source_encodings = self.downsample(source_encodings.transpose(1,2)).transpose(1,2) # Don't have to change\n",
        "\n",
        "        ######################## YOUR CODE ######################\n",
        "        # You need to implement the code for the rest of the part (LSTM, logit, ...)\n",
        "        #\n",
        "        # And fill in the below loss code for calling CTCLoss\n",
        "        # Tip: check argument types/shapes in https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "        loss = TODO\n",
        "        #########################################################\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def predict(self, input_values, attention_mask=None):\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_values)\n",
        "\n",
        "        ######################## YOUR CODE ######################\n",
        "        # fill in the inference code. This should be the same as \"forward\" function except loss calculation.\n",
        "        #########################################################\n",
        "\n",
        "        pred_texts = self.ctc_decoder(source_encodings.cpu(), source_lengths.cpu())\n",
        "        pred_texts = [self.token_processor.decode(pred_text[0].tokens) for pred_text in pred_texts]\n",
        "        return pred_texts, source_encodings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoBd7QLfDOJs"
      },
      "source": [
        "## 4. Training\n",
        "\n",
        "Training ASR model usually takes long time and consumes a lot of GPU space. Here, we won't ask to reach convergence to get very low WER (try if you have enough capacity in Colab!).\n",
        "\n",
        "Below, we put very minimal setting for training, in which we expect to see below 50% WER when the model is trained for 20 epochs, if successfully implemented. As mentioned, the fine-tuning can get below 20% WER. Also, the loss may seem not improving for a few first epochs, which is natural for CTC. So be patient and please try early, so you can try many.\n",
        "\n",
        "This procedure takes several hours. \n",
        "\n",
        "NOTE - The choise of layer used for training ASR will be crucial. Try training the model with different layers. \n",
        "\n",
        "The points distribution based on WER on the test set:\n",
        "\n",
        "< 70% : 10 points\n",
        "\n",
        "< 50% : 15 points\n",
        "\n",
        "< 45% : 20 points\n",
        "\n",
        "< 40% : 25 points\n",
        "\n",
        "< 35% : 30 points (Full points) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2eet5tOl2fB"
      },
      "outputs": [],
      "source": [
        "## Helper functions for WER/CER calculation and train/validation loop.\n",
        "\n",
        "def get_wer(gt_text, pred_text):\n",
        "    gt_text_tokens = [t for t in gt_text.upper().split(' ') if t != '']\n",
        "    pred_text_tokens = [t for t in pred_text.upper().split(' ') if t != '']\n",
        "    return torchaudio.functional.edit_distance(pred_text_tokens,gt_text_tokens) /len(gt_text_tokens)\n",
        "\n",
        "def get_cer(gt_text, pred_text):\n",
        "    return torchaudio.functional.edit_distance(pred_text.upper(),gt_text.upper()) /len(gt_text)\n",
        "\n",
        "\n",
        "def train_loop(dataloader, model, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    report_step = np.ceil(len(dataloader)/5).astype(int)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch_i, batch in enumerate(dataloader):\n",
        "        input_values = batch['input_values'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        text = batch['text']\n",
        "        # Compute loss\n",
        "        loss = model(input_values, text, attention_mask)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch_i % report_step == 0:\n",
        "            loss, current = loss.item(), batch_i * batch_size + len(input_values)\n",
        "            print(f\"    Train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    wer, cer = 0, 0\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_values = batch['input_values'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            gt_texts = batch['text']\n",
        "            pred_texts, _ = model.predict(input_values, attention_mask)\n",
        "            for gt_text, pred_text in zip(gt_texts, pred_texts):\n",
        "                wer += get_wer(gt_text, pred_text)\n",
        "                cer += get_cer(gt_text, pred_text)\n",
        "                cnt += 1\n",
        "\n",
        "    wer = wer/cnt\n",
        "    cer = cer/cnt\n",
        "    return wer, cer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HEpJ1I0fziae",
        "outputId": "c4c862ff-dc9f-416d-9743-d252f48339f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "# If False, please check the runtime type of the session.\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSQ3yFwrz_iV"
      },
      "outputs": [],
      "source": [
        "## Training config\n",
        "\n",
        "device = 'cuda'\n",
        "batch_size = 4 # You may meet GPU limit if you put this number higher.\n",
        "epoch = 20 # This number is not enough for convergence but sufficient to pass the assignment.\n",
        "lr = 4e-5 # this should be sufficiently small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTKY4ou7zAYq",
        "outputId": "2880a40c-6bc2-449c-a0df-ae4fc7d60ef5"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = ASR()\n",
        "model = model.to(device).train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTLonDcSzJX-",
        "outputId": "44edcbcf-a58a-4039-b00a-7d50ee9cb580"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Initialize dataloader\n",
        "\n",
        "train_dataset = SpeechTextDataset(DATA_DIR, 'train')\n",
        "val_dataset = SpeechTextDataset(DATA_DIR, 'dev')\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=2,\n",
        "                        drop_last=True,\n",
        "                        pin_memory=True,\n",
        "                        collate_fn=SpeechTextDataset.collate)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=2,\n",
        "                        drop_last=False,\n",
        "                        pin_memory=True,\n",
        "                        collate_fn=SpeechTextDataset.collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sai1HEuQzreu"
      },
      "outputs": [],
      "source": [
        "# Set up optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq-jAbGa0FC8"
      },
      "outputs": [],
      "source": [
        "best_weights = model.state_dict()\n",
        "best_wer = 99999\n",
        "for e in range(epoch):\n",
        "    print(f\"Epoch [{e+1}/{epoch}] - {datetime.now()}\")\n",
        "    train_loop(train_dataloader, model, optimizer)\n",
        "    val_wer, val_cer = test_loop(val_dataloader, model)\n",
        "    print(f\"    Validation WER: {(100*val_wer):>0.2f}%, CER: {(100*val_cer):>0.2f}%\")\n",
        "    if val_wer < best_wer:\n",
        "        best_weights = model.state_dict()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpyrJr2O2iJ3"
      },
      "outputs": [],
      "source": [
        "# save model checkpoint\n",
        "\n",
        "torch.save({'state_dict':model.state_dict()}, \"best_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O_011aS6jgD"
      },
      "outputs": [],
      "source": [
        "# Load your best model\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", map_location=\"cpu\")['state_dict'])\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = SpeechTextDataset(DATA_DIR, 'test')\n",
        "submission_order_file = Path(DATA_DIR) / \"test_submission_order.txt\"\n",
        "\n",
        "# Generate test transcriptions\n",
        "model.eval()\n",
        "test_transcriptions = {}\n",
        "\n",
        "print(\"Generating test transcriptions...\")\n",
        "with torch.no_grad():\n",
        "    for i in tqdm.tqdm(range(len(test_dataset)), desc=\"Transcribing test set\"):\n",
        "        sample = test_dataset[i]\n",
        "        wav_file = test_dataset.wav_files[i]\n",
        "        filename = wav_file.stem\n",
        "        \n",
        "        input_values = sample['wav'].unsqueeze(0).to(device)\n",
        "        attention_mask = torch.ones_like(input_values).to(device)\n",
        "        \n",
        "        pred_texts, _ = model.predict(input_values, attention_mask)\n",
        "        prediction = pred_texts[0] if len(pred_texts) > 0 else \"\"\n",
        "        test_transcriptions[filename] = prediction\n",
        "\n",
        "# Create submission file in correct order\n",
        "with open(submission_order_file, 'r') as f:\n",
        "    submission_order = [line.strip() for line in f.readlines()]\n",
        "\n",
        "output_lines = []\n",
        "for audio_id in submission_order:\n",
        "    if audio_id in test_transcriptions:\n",
        "        output_lines.append(f\"{audio_id}|{test_transcriptions[audio_id]}\")\n",
        "    else:\n",
        "        output_lines.append(f\"{audio_id}|\")\n",
        "\n",
        "with open(\"asr_submission.txt\", \"w\") as f:\n",
        "    for line in output_lines:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "print(\"Test transcriptions saved to asr_submission.txt\")\n",
        "\n",
        "### !!!! Please make sure that the output file format is what's expected."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
